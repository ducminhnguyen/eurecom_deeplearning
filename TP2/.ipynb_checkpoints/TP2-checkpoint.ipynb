{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Student 1:</b> THAN Thanh An  \n",
    "<b> Student 2:</b> NGUYEN Minh Duc\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by May 29th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab2]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the last Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%.  Can  you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks:  **LeNet-5** to go to  more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell above to load the MNIST data that comes  with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example :\n",
    "**y=softmax(Wx+b)** seen in the DeepLearing course last week. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the tensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to  visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 2.281444597\n",
      "Epoch:  02   =====> Loss= 2.229430199\n",
      "Epoch:  03   =====> Loss= 2.179733610\n",
      "Epoch:  04   =====> Loss= 2.132062387\n",
      "Epoch:  05   =====> Loss= 2.086399126\n",
      "Epoch:  06   =====> Loss= 2.042266178\n",
      "Epoch:  07   =====> Loss= 1.999297953\n",
      "Epoch:  08   =====> Loss= 1.958605456\n",
      "Epoch:  09   =====> Loss= 1.919204998\n",
      "Epoch:  10   =====> Loss= 1.881934905\n",
      "Epoch:  11   =====> Loss= 1.844592929\n",
      "Epoch:  12   =====> Loss= 1.809497356\n",
      "Epoch:  13   =====> Loss= 1.775510406\n",
      "Epoch:  14   =====> Loss= 1.742494106\n",
      "Epoch:  15   =====> Loss= 1.711519384\n",
      "Epoch:  16   =====> Loss= 1.680916739\n",
      "Epoch:  17   =====> Loss= 1.652868390\n",
      "Epoch:  18   =====> Loss= 1.624090815\n",
      "Epoch:  19   =====> Loss= 1.596414638\n",
      "Epoch:  20   =====> Loss= 1.569908214\n",
      "Epoch:  21   =====> Loss= 1.545104074\n",
      "Epoch:  22   =====> Loss= 1.522130179\n",
      "Epoch:  23   =====> Loss= 1.498071003\n",
      "Epoch:  24   =====> Loss= 1.476168418\n",
      "Epoch:  25   =====> Loss= 1.453928041\n",
      "Epoch:  26   =====> Loss= 1.432838893\n",
      "Epoch:  27   =====> Loss= 1.412847590\n",
      "Epoch:  28   =====> Loss= 1.393792963\n",
      "Epoch:  29   =====> Loss= 1.373756194\n",
      "Epoch:  30   =====> Loss= 1.356125402\n",
      "Epoch:  31   =====> Loss= 1.338076568\n",
      "Epoch:  32   =====> Loss= 1.321931577\n",
      "Epoch:  33   =====> Loss= 1.303276753\n",
      "Epoch:  34   =====> Loss= 1.288472319\n",
      "Epoch:  35   =====> Loss= 1.272253036\n",
      "Epoch:  36   =====> Loss= 1.258300662\n",
      "Epoch:  37   =====> Loss= 1.243888187\n",
      "Epoch:  38   =====> Loss= 1.230394268\n",
      "Epoch:  39   =====> Loss= 1.216255546\n",
      "Epoch:  40   =====> Loss= 1.202849865\n",
      "Epoch:  41   =====> Loss= 1.189921880\n",
      "Epoch:  42   =====> Loss= 1.177939320\n",
      "Epoch:  43   =====> Loss= 1.167502642\n",
      "Epoch:  44   =====> Loss= 1.155287790\n",
      "Epoch:  45   =====> Loss= 1.142962837\n",
      "Epoch:  46   =====> Loss= 1.133818865\n",
      "Epoch:  47   =====> Loss= 1.122161317\n",
      "Epoch:  48   =====> Loss= 1.111836290\n",
      "Epoch:  49   =====> Loss= 1.101030517\n",
      "Epoch:  50   =====> Loss= 1.091049600\n",
      "Epoch:  51   =====> Loss= 1.080644822\n",
      "Epoch:  52   =====> Loss= 1.070801330\n",
      "Epoch:  53   =====> Loss= 1.064041853\n",
      "Epoch:  54   =====> Loss= 1.054335928\n",
      "Epoch:  55   =====> Loss= 1.044709110\n",
      "Epoch:  56   =====> Loss= 1.038246059\n",
      "Epoch:  57   =====> Loss= 1.029135680\n",
      "Epoch:  58   =====> Loss= 1.020601439\n",
      "Epoch:  59   =====> Loss= 1.013166308\n",
      "Epoch:  60   =====> Loss= 1.005741310\n",
      "Epoch:  61   =====> Loss= 0.997515345\n",
      "Epoch:  62   =====> Loss= 0.989891255\n",
      "Epoch:  63   =====> Loss= 0.984211004\n",
      "Epoch:  64   =====> Loss= 0.975396430\n",
      "Epoch:  65   =====> Loss= 0.970343113\n",
      "Epoch:  66   =====> Loss= 0.964370632\n",
      "Epoch:  67   =====> Loss= 0.956870902\n",
      "Epoch:  68   =====> Loss= 0.950960600\n",
      "Epoch:  69   =====> Loss= 0.944080853\n",
      "Epoch:  70   =====> Loss= 0.938812149\n",
      "Epoch:  71   =====> Loss= 0.934145010\n",
      "Epoch:  72   =====> Loss= 0.926689434\n",
      "Epoch:  73   =====> Loss= 0.921355832\n",
      "Epoch:  74   =====> Loss= 0.915849364\n",
      "Epoch:  75   =====> Loss= 0.909962666\n",
      "Epoch:  76   =====> Loss= 0.903241420\n",
      "Epoch:  77   =====> Loss= 0.900187540\n",
      "Epoch:  78   =====> Loss= 0.894807947\n",
      "Epoch:  79   =====> Loss= 0.890662372\n",
      "Epoch:  80   =====> Loss= 0.886234140\n",
      "Epoch:  81   =====> Loss= 0.879927254\n",
      "Epoch:  82   =====> Loss= 0.875048840\n",
      "Epoch:  83   =====> Loss= 0.872208095\n",
      "Epoch:  84   =====> Loss= 0.865784752\n",
      "Epoch:  85   =====> Loss= 0.862360907\n",
      "Epoch:  86   =====> Loss= 0.857314134\n",
      "Epoch:  87   =====> Loss= 0.852557778\n",
      "Epoch:  88   =====> Loss= 0.849093258\n",
      "Epoch:  89   =====> Loss= 0.844559908\n",
      "Epoch:  90   =====> Loss= 0.842405951\n",
      "Epoch:  91   =====> Loss= 0.836764979\n",
      "Epoch:  92   =====> Loss= 0.832512188\n",
      "Epoch:  93   =====> Loss= 0.829375160\n",
      "Epoch:  94   =====> Loss= 0.825072765\n",
      "Epoch:  95   =====> Loss= 0.821623051\n",
      "Epoch:  96   =====> Loss= 0.817424405\n",
      "Epoch:  97   =====> Loss= 0.813314581\n",
      "Epoch:  98   =====> Loss= 0.811318135\n",
      "Epoch:  99   =====> Loss= 0.808147621\n",
      "Epoch:  100   =====> Loss= 0.804041159\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8508\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 10000\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "#             print(type(batch_xs))\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Go to the **TP2** folder, \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir= log_files/\"**, it will generate an http link ,ex http://666.6.6.6:6006,\n",
    "- Copy this  link into your web browser \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One you are now familar with **tensorFlow** and **tensorBoard**, you are in this section to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "In more advanced step you will make some optimizations to get more than 99% of accuracy. The best model can get to over 99.7% accuracy! \n",
    "\n",
    "For more information, have a look at this list of results : http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet 5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6 **Activation.** sigmoid **Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16. **Activation.** sigmoid **Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use **flatten*  from tensorflow.contrib.layers import flatten\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 5: Fully Connected.** This should have 10 outputs. **Activation.** softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Helper functions  for weigths and bias initilization \n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, mean=0, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LeNet5_Model(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = bias_variable([6])\n",
    "    conv1_pre   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1_ac = tf.nn.sigmoid(conv1_pre)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = bias_variable([16])\n",
    "    conv2_pre   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2_ac = tf.nn.sigmoid(conv2_pre)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1_pre   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1   = tf.nn.sigmoid(fc1_pre)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = bias_variable([84])\n",
    "    fc2_pre    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.sigmoid(fc2_pre)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "    fc3_b  = bias_variable([10])\n",
    "    fc3_pre = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    \n",
    "    logits = fc3_pre\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The number of parameter of the model is the number of connection in the neural network. However, this neural network have convolution layers which has weight sharing mechanism so the true number of parameter is much lower than a fully connected neural network.\n",
    "\n",
    "This network has trainable parameter in the convolution layers which are: \n",
    "- Layer 1: [5, 5, 1, 6] + [6] = 156 parameters\n",
    "- Layer 2: [5, 5, 6, 16] + [16] = 2416 parameters\n",
    "- Layer 3: [400, 120] + [120] = 48120 parameters\n",
    "- Layer 4: [120, 84] + [84] = 10164 parameters\n",
    "- Layer 5: [84, 10] + [10] = 850 parameters\n",
    "=> In total: 61706 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.1.3. </b>  Start the training with the parameters cited below:\n",
    "\n",
    "     Learning rate =0.1\n",
    "     Loss Fucntion : Cross entropy\n",
    "     Optimisateur: SGD\n",
    "     Number of training iterations= 10000\n",
    "     The batch size =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "x_new = tf.map_fn(lambda x1: tf.image.pad_to_bounding_box(x1, 2, 2, 32, 32), x)\n",
    "\n",
    "# Defined model\n",
    "model = LeNet5_Model(x_new)\n",
    "\n",
    "# cost function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimization \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "('EPOCH 1 - ', 'Validation Accuracy = 0.110')\n",
      "('EPOCH 2 - ', 'Validation Accuracy = 0.098')\n",
      "('EPOCH 3 - ', 'Validation Accuracy = 0.098')\n",
      "('EPOCH 4 - ', 'Validation Accuracy = 0.110')\n",
      "('EPOCH 5 - ', 'Validation Accuracy = 0.113')\n",
      "('EPOCH 6 - ', 'Validation Accuracy = 0.110')\n",
      "('EPOCH 7 - ', 'Validation Accuracy = 0.113')\n",
      "('EPOCH 8 - ', 'Validation Accuracy = 0.113')\n",
      "('EPOCH 9 - ', 'Validation Accuracy = 0.257')\n",
      "('EPOCH 10 - ', 'Validation Accuracy = 0.641')\n",
      "('EPOCH 11 - ', 'Validation Accuracy = 0.808')\n",
      "('EPOCH 12 - ', 'Validation Accuracy = 0.872')\n",
      "('EPOCH 13 - ', 'Validation Accuracy = 0.900')\n",
      "('EPOCH 14 - ', 'Validation Accuracy = 0.917')\n",
      "('EPOCH 15 - ', 'Validation Accuracy = 0.930')\n",
      "('EPOCH 16 - ', 'Validation Accuracy = 0.947')\n",
      "('EPOCH 17 - ', 'Validation Accuracy = 0.951')\n",
      "('EPOCH 18 - ', 'Validation Accuracy = 0.957')\n",
      "('EPOCH 19 - ', 'Validation Accuracy = 0.959')\n",
      "('EPOCH 20 - ', 'Validation Accuracy = 0.962')\n",
      "('EPOCH 21 - ', 'Validation Accuracy = 0.965')\n",
      "('EPOCH 22 - ', 'Validation Accuracy = 0.966')\n",
      "('EPOCH 23 - ', 'Validation Accuracy = 0.969')\n",
      "('EPOCH 24 - ', 'Validation Accuracy = 0.971')\n",
      "('EPOCH 25 - ', 'Validation Accuracy = 0.973')\n",
      "('EPOCH 26 - ', 'Validation Accuracy = 0.974')\n",
      "('EPOCH 27 - ', 'Validation Accuracy = 0.973')\n",
      "('EPOCH 28 - ', 'Validation Accuracy = 0.973')\n",
      "('EPOCH 29 - ', 'Validation Accuracy = 0.975')\n",
      "('EPOCH 30 - ', 'Validation Accuracy = 0.978')\n",
      "('EPOCH 31 - ', 'Validation Accuracy = 0.976')\n",
      "('EPOCH 32 - ', 'Validation Accuracy = 0.979')\n",
      "('EPOCH 33 - ', 'Validation Accuracy = 0.977')\n",
      "('EPOCH 34 - ', 'Validation Accuracy = 0.979')\n",
      "('EPOCH 35 - ', 'Validation Accuracy = 0.979')\n",
      "('EPOCH 36 - ', 'Validation Accuracy = 0.980')\n",
      "('EPOCH 37 - ', 'Validation Accuracy = 0.977')\n",
      "('EPOCH 38 - ', 'Validation Accuracy = 0.980')\n",
      "('EPOCH 39 - ', 'Validation Accuracy = 0.981')\n",
      "('EPOCH 40 - ', 'Validation Accuracy = 0.978')\n",
      "('EPOCH 41 - ', 'Validation Accuracy = 0.982')\n",
      "('EPOCH 42 - ', 'Validation Accuracy = 0.982')\n",
      "('EPOCH 43 - ', 'Validation Accuracy = 0.981')\n",
      "('EPOCH 44 - ', 'Validation Accuracy = 0.979')\n",
      "('EPOCH 45 - ', 'Validation Accuracy = 0.982')\n",
      "('EPOCH 46 - ', 'Validation Accuracy = 0.984')\n",
      "('EPOCH 47 - ', 'Validation Accuracy = 0.983')\n",
      "('EPOCH 48 - ', 'Validation Accuracy = 0.982')\n",
      "('EPOCH 49 - ', 'Validation Accuracy = 0.984')\n",
      "('EPOCH 50 - ', 'Validation Accuracy = 0.984')\n",
      "('EPOCH 51 - ', 'Validation Accuracy = 0.984')\n",
      "('EPOCH 52 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 53 - ', 'Validation Accuracy = 0.986')\n",
      "('EPOCH 54 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 55 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 56 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 57 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 58 - ', 'Validation Accuracy = 0.986')\n",
      "('EPOCH 59 - ', 'Validation Accuracy = 0.986')\n",
      "('EPOCH 60 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 61 - ', 'Validation Accuracy = 0.985')\n",
      "('EPOCH 62 - ', 'Validation Accuracy = 0.986')\n",
      "('EPOCH 63 - ', 'Validation Accuracy = 0.986')\n",
      "('EPOCH 64 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 65 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 66 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 67 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 68 - ', 'Validation Accuracy = 0.986')\n",
      "('EPOCH 69 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 70 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 71 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 72 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 73 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 74 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 75 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 76 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 77 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 78 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 79 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 80 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 81 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 82 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 83 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 84 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 85 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 86 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 87 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 88 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 89 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 90 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 91 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 92 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 93 - ', 'Validation Accuracy = 0.989')\n",
      "('EPOCH 94 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 95 - ', 'Validation Accuracy = 0.987')\n",
      "('EPOCH 96 - ', 'Validation Accuracy = 0.989')\n",
      "('EPOCH 97 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 98 - ', 'Validation Accuracy = 0.988')\n",
      "('EPOCH 99 - ', 'Validation Accuracy = 0.989')\n",
      "('EPOCH 100 - ', 'Validation Accuracy = 0.988')\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for i in range(training_epochs):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        \n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            batch_x = batch_x.reshape([-1,28,28,1])\n",
    "            \n",
    "            # start session\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation, sess)\n",
    "        print(\"EPOCH {} - \".format(i+1),\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        \n",
    "    saver.save(sess, 'lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(xTest, yTest, sess):\n",
    "    xTest = xTest.reshape([-1,28,28,1])\n",
    "    num_examples = len(xTest)\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x, batch_y = xTest[offset:offset+batch_size], yTest[offset:offset+batch_size]\n",
    "        \n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    \n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "import time\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    # the structure of this network is already defined\n",
    "    # param to change: isdropout, \n",
    "#  Learning rate =0.1\n",
    "#  Loss Fucntion: Cross entropy\n",
    "#  Optimisateur: SGD\n",
    "#  Number of training iterations= 10000\n",
    "#  The batch size =128\n",
    "    def __init__(self, LR=0.1, Iter=100, Batchsize=128, display_step = 5, \n",
    "                 Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.sigmoid, NN_Name=\"\"):\n",
    "        self.logs_path = \"log_files/\"\n",
    "        self.display_step = display_step\n",
    "        self.LR = LR\n",
    "        self.Iter = Iter\n",
    "        self.Batchsize = Batchsize\n",
    "        self.Optimizer = Optimizer\n",
    "        self.IsDropOut = IsDropOut\n",
    "        self.Activation = Activation\n",
    "        self.NN_Name = NN_Name\n",
    "        \n",
    "        \n",
    "        # x_normalize = tf.divide(x_new, 255.0)\n",
    "#         cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y)\n",
    "#         loss_operation = tf.reduce_mean(cross_entropy)\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "#         training_operation = optimizer.minimize(loss_operation)\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(NN_Name + '_Model'):\n",
    "            # Model\n",
    "            self.model = self.define_model(x, y)\n",
    "        with tf.name_scope(NN_Name + '_Loss'):\n",
    "            # Minimize error using cross entropy\n",
    "            self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.model, labels=y)\n",
    "            self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "        with tf.name_scope(NN_Name + '_Optimizer'):\n",
    "            # Gradient Descent\n",
    "            self.tfoptimizer = self.Optimizer(self.LR).minimize(self.cost)\n",
    "        with tf.name_scope(NN_Name + '_Accuracy'):\n",
    "            # Accuracy\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.model, 1), tf.argmax(y, 1))\n",
    "            self.accuracy_operation = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        # Create a summary to monitor cost tensor\n",
    "        self.summary_cost_epoch = tf.summary.scalar(NN_Name + \"_Loss_Epoch\", self.cost)\n",
    "        self.summary_cost_batch = tf.summary.scalar(NN_Name + \"_Loss_Batch\", self.cost)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        self.summary_train_acc = tf.summary.scalar(NN_Name + \"_Train_Accuracy\", self.accuracy_operation)\n",
    "        self.summary_validation_acc = tf.summary.scalar(NN_Name + \"_Validation_Accuracy\", self.accuracy_operation)\n",
    "        # Merge all summaries into a single op\n",
    "    \n",
    "    def define_model(self, x, y):\n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "        with tf.name_scope(self.NN_Name + \"_Layer_1\"):\n",
    "        # resized image input 28x28x1 => 32x32x1\n",
    "            x_new = tf.map_fn(lambda x1: tf.image.pad_to_bounding_box(x1, 2, 2, 32, 32), x)\n",
    "\n",
    "            # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "            conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "            conv1_b = bias_variable([6])\n",
    "            conv1_pre   = tf.nn.conv2d(x_new, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            conv1_ac = self.Activation(conv1_pre)\n",
    "\n",
    "            # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "            conv1 = tf.nn.max_pool(conv1_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "        with tf.name_scope(self.NN_Name + \"_Layer_2\"):\n",
    "            conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "            conv2_b = bias_variable([16])\n",
    "            conv2_pre   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            conv2_ac = self.Activation(conv2_pre)\n",
    "\n",
    "            # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "            conv2 = tf.nn.max_pool(conv2_ac, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "        # SOLUTION: Flatten. Input = 5x5x16. Output = 400.t multiple values for keyword argument 'LR\n",
    "        with tf.name_scope(self.NN_Name + \"_Flatten\"):\n",
    "            fc0   = flatten(conv2)\n",
    "        \n",
    "        # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        with tf.name_scope(self.NN_Name + \"_Layer_3\"):\n",
    "            fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "            fc1_b = tf.Variable(tf.zeros(120))\n",
    "            fc1_pre   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "            # SOLUTION: Activation.\n",
    "            fc1   = self.Activation(fc1_pre)\n",
    "        \n",
    "        fc1_out = None\n",
    "        # keep probability number to be input in training\n",
    "        # drop out\n",
    "        if (self.IsDropOut):\n",
    "            with tf.name_scope(self.NN_Name + \"_Dropout_1\"):\n",
    "                fc1_out = tf.nn.dropout(fc1, keep_prob)\n",
    "        else:\n",
    "            fc1_out = fc1\n",
    "\n",
    "        # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        with tf.name_scope(self.NN_Name + \"_Layer_4\"):\n",
    "            fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "            fc2_b  = bias_variable([84])\n",
    "            fc2_pre = tf.matmul(fc1_out, fc2_W) + fc2_b\n",
    "\n",
    "        # SOLUTION: Activation.\n",
    "            fc2    = self.Activation(fc2_pre)\n",
    "        \n",
    "        fc2_out = None\n",
    "        if (self.IsDropOut):\n",
    "            with tf.name_scope(self.NN_Name + \"_Dropout_2\"):\n",
    "                fc2_out = tf.nn.dropout(fc2, keep_prob)\n",
    "        else:\n",
    "            fc2_out = fc2\n",
    "            \n",
    "        # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "        with tf.name_scope(self.NN_Name + \"_Layer_5\"):\n",
    "            fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "            fc3_b  = bias_variable([10])\n",
    "            fc3_pre = tf.matmul(fc2_out, fc3_W) + fc3_b\n",
    "\n",
    "            logits = fc3_pre\n",
    "        return logits\n",
    "        \n",
    "    def evaluation(self, X_val, y_val, sess, acc_summary=None):\n",
    "        X_val_reshaped = X_val.reshape([-1, 28, 28, 1])\n",
    "        accuracy = 0\n",
    "        if (acc_summary != None):\n",
    "            if (self.IsDropOut):\n",
    "                accuracy, summary_ = sess.run([self.accuracy_operation, acc_summary], feed_dict={x: X_val_reshaped, y: y_val, keep_prob: 1.0})\n",
    "            else:\n",
    "                accuracy, summary_ = sess.run([self.accuracy_operation, acc_summary], feed_dict={x: X_val_reshaped, y: y_val})\n",
    "            return accuracy, summary_\n",
    "        else:\n",
    "            if (self.IsDropOut):\n",
    "                accuracy = sess.run([self.accuracy_operation], feed_dict={x: X_val_reshaped, y: y_val, keep_prob: 1.0})\n",
    "            else:\n",
    "                accuracy = sess.run([self.accuracy_operation], feed_dict={x: X_val_reshaped, y: y_val})\n",
    "            return accuracy\n",
    "        \n",
    "    def train(self, X_train, y_train, X_validation, y_validation):\n",
    "        # Initializing the session \n",
    "        print (\"Start Training!\" + self.NN_Name)\n",
    "        ####\n",
    "#         X_train, y_train = shuffle(X_train, y_train)\n",
    "#         for offset in range(0, num_examples, batch_size):\n",
    "#             end = offset + batch_size\n",
    "#             batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "#             batch_x = batch_x.reshape([-1,28,28,1])\n",
    "            \n",
    "            \n",
    "#             sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "# #         validation_accuracy = evaluate(X_validation, y_validation, sess)\n",
    "#         validation_accuracy = evaluate(X_validation, y_validation, sess)\n",
    "#         print(\"EPOCH {} ...\".format(i+1))\n",
    "#         print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        # Launch the graph for training\n",
    "        with tf.Session() as sess:\n",
    "            sess.close()\n",
    "            sess.run(self.init)\n",
    "            # op to write logs to Tensorboard\n",
    "            summary_writer = tf.summary.FileWriter(self.logs_path, graph=tf.get_default_graph())\n",
    "            # Training cycle\n",
    "            start = time.time()\n",
    "            for epoch in np.arange(self.Iter):\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(mnist.train.num_examples/self.Batchsize)\n",
    "                X_train, y_train = shuffle(X_train, y_train)\n",
    "                \n",
    "                # Loop over all batches\n",
    "                for offset in range(0, mnist.train.num_examples, self.Batchsize):\n",
    "                    end = offset + self.Batchsize\n",
    "                    batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                    batch_x = batch_x.reshape([-1,28,28,1])\n",
    "                    # Run optimization op (backprop), cost op (to get loss value)\n",
    "                    # and summary nodes\n",
    "        #             print(type(batch_xs))\n",
    "                    c = None\n",
    "                    if (self.IsDropOut == False):\n",
    "                        _, c = sess.run([self.tfoptimizer, self.cost], \n",
    "                                                 feed_dict={x: batch_x, y: batch_y})\n",
    "                    else:\n",
    "                        _, c = sess.run([self.tfoptimizer, self.cost], \n",
    "                                                 feed_dict={x: batch_x, y: batch_y, keep_prob: 0.75})\n",
    "                    # Write logs at every iteration\n",
    "#                     summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                    # fix this\n",
    "                    # Compute average loss\n",
    "#                     print(\"Batch \", offset / batch_size + 1 , \" loss: \", c)\n",
    "                    avg_cost += c / total_batch\n",
    "                # Display logs per epoch step\n",
    "                \n",
    "                if (epoch+1) % self.display_step == 0:\n",
    "                    accu, accu_sum = self.evaluation(X_validation, y_validation, sess, self.summary_validation_acc)\n",
    "                    print(\"Epoch: \", '%02d' % (epoch+1), \n",
    "                          \", Loss=\", \"{:.9f}\".format(avg_cost), \n",
    "#                           \", Training accuracy=\", self.evaluation(X_train, y_train, sess),\n",
    "                          \", Validation accuracy=\", accu\n",
    "                         )\n",
    "                    summary_writer.add_summary(accu_sum, epoch)\n",
    "                else:\n",
    "                    print(\"Epoch: \", '%02d' % (epoch+1), \n",
    "                          \", Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "                    \n",
    "            print(\"Optimization Finished! in %f seconds\"%(time.time()-start))\n",
    "            # Save model\n",
    "            self.saver.save(sess, \"mnist_nn_\" + str(self.Iter) + \"_\" + str(time.time()), global_step=100)\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Accuracy on test data: \", self.evaluation(mnist.test.images, mnist.test.labels, sess, None))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!SGD_Adam_0.001_50_128_test\n",
      "('Epoch: ', '01', ', Loss=', '0.352833678')\n",
      "('Epoch: ', '02', ', Loss=', '0.090233625')\n",
      "('Epoch: ', '03', ', Loss=', '0.062268283')\n",
      "Optimization Finished! in 66.787133 seconds\n",
      "('Accuracy on test data: ', [0.98449999])\n"
     ]
    }
   ],
   "source": [
    "mnist_nn = NeuralNetwork(LR=0.001, Iter=3, Batchsize=128, \n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=False, Activation=tf.nn.relu, NN_Name=\"SGD_Adam_0.001_50_128_test\")\n",
    "mnist_nn.train(X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!SGD_Adam_0.001_50_128_test\n",
      "('Epoch: ', '01', ', Loss=', '0.345587628')\n",
      "('Epoch: ', '02', ', Loss=', '0.096521974')\n",
      "('Epoch: ', '03', ', Loss=', '0.066271103')\n",
      "('Epoch: ', '04', ', Loss=', '0.052491907')\n",
      "('Epoch: ', '05', ', Loss=', '0.042587947', ', Validation accuracy=', 0.9874)\n",
      "('Epoch: ', '06', ', Loss=', '0.036022776')\n",
      "('Epoch: ', '07', ', Loss=', '0.029734509')\n",
      "('Epoch: ', '08', ', Loss=', '0.026426863')\n",
      "('Epoch: ', '09', ', Loss=', '0.022758829')\n",
      "('Epoch: ', '10', ', Loss=', '0.019689096', ', Validation accuracy=', 0.98979998)\n",
      "('Epoch: ', '11', ', Loss=', '0.016455605')\n",
      "('Epoch: ', '12', ', Loss=', '0.016069259')\n",
      "('Epoch: ', '13', ', Loss=', '0.014323767')\n",
      "('Epoch: ', '14', ', Loss=', '0.011549209')\n",
      "('Epoch: ', '15', ', Loss=', '0.011597225', ', Validation accuracy=', 0.98860002)\n",
      "('Epoch: ', '16', ', Loss=', '0.010473550')\n",
      "('Epoch: ', '17', ', Loss=', '0.008324415')\n",
      "('Epoch: ', '18', ', Loss=', '0.007818341')\n",
      "('Epoch: ', '19', ', Loss=', '0.009787666')\n",
      "('Epoch: ', '20', ', Loss=', '0.006651273', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '21', ', Loss=', '0.004827737')\n",
      "('Epoch: ', '22', ', Loss=', '0.005939334')\n",
      "('Epoch: ', '23', ', Loss=', '0.008987649')\n",
      "('Epoch: ', '24', ', Loss=', '0.004493293')\n",
      "('Epoch: ', '25', ', Loss=', '0.003984208', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '26', ', Loss=', '0.006237006')\n",
      "('Epoch: ', '27', ', Loss=', '0.006047612')\n",
      "('Epoch: ', '28', ', Loss=', '0.004950416')\n",
      "('Epoch: ', '29', ', Loss=', '0.004946245')\n",
      "('Epoch: ', '30', ', Loss=', '0.003598209', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '31', ', Loss=', '0.002511087')\n",
      "('Epoch: ', '32', ', Loss=', '0.007011402')\n",
      "('Epoch: ', '33', ', Loss=', '0.006877672')\n",
      "('Epoch: ', '34', ', Loss=', '0.002087681')\n",
      "('Epoch: ', '35', ', Loss=', '0.002737200', ', Validation accuracy=', 0.99040002)\n",
      "('Epoch: ', '36', ', Loss=', '0.002735825')\n",
      "('Epoch: ', '37', ', Loss=', '0.003453234')\n",
      "('Epoch: ', '38', ', Loss=', '0.006531069')\n",
      "('Epoch: ', '39', ', Loss=', '0.003027576')\n",
      "('Epoch: ', '40', ', Loss=', '0.003096190', ', Validation accuracy=', 0.99080002)\n",
      "('Epoch: ', '41', ', Loss=', '0.003327939')\n",
      "('Epoch: ', '42', ', Loss=', '0.004205997')\n",
      "('Epoch: ', '43', ', Loss=', '0.001149182')\n",
      "('Epoch: ', '44', ', Loss=', '0.004208883')\n",
      "('Epoch: ', '45', ', Loss=', '0.002595066', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '46', ', Loss=', '0.002154403')\n",
      "('Epoch: ', '47', ', Loss=', '0.005277542')\n",
      "('Epoch: ', '48', ', Loss=', '0.003189990')\n",
      "('Epoch: ', '49', ', Loss=', '0.003034504')\n",
      "('Epoch: ', '50', ', Loss=', '0.002718110', ', Validation accuracy=', 0.99019998)\n",
      "Optimization Finished! in 1186.329944 seconds\n",
      "('Accuracy on test data: ', [0.99049997])\n"
     ]
    }
   ],
   "source": [
    "mnist_nn = NeuralNetwork(LR=0.001, Iter=50, Batchsize=128, \n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=False, Activation=tf.nn.relu, NN_Name=\"SGD_Adam_0.001_50_128_test\")\n",
    "mnist_nn.train(X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.1.6 </b> : Use tensorBoard to visualise and save the LeNet5 Graph and all learning curves. \n",
    "Save all obtained figures in the folder **\"TP2/MNIST_99_Challenge_Figures\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#  insert your obtained figure here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your answer goas here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>  Change the sigmoid function with a Relu :\n",
    "\n",
    "- Retrain your network with SGD and AdamOptimizer and then fill the table above  :\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent         |AdamOptimizer |\n",
    "| -------------        |: -------------: | ---------:   \n",
    "| Validation Accuracy  |         |    |      \n",
    "| Testing Accuracy     |           |    |       \n",
    "| Training Time        |           |        |  |  \n",
    "\n",
    "\n",
    "- Try with different learning rates for each Optimizer (0.0001 and 0.001 ) and different Batch sizes (50 and 128) for 20000 Epochs. \n",
    "\n",
    "- For each optimizer, plot (on the same curve) the **testing accuracies** function to **(learning rate, batch size)** \n",
    "\n",
    "\n",
    "\n",
    "- Did you reach the 99% accuracy ? What are the optimal parametres that gave you the best results? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "LR=0.001 \t- Batchsize =50 \t - GradientDescent\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '2.279870088')\n",
      "('Epoch: ', '02', ', Loss=', '2.110104638')\n",
      "('Epoch: ', '03', ', Loss=', '1.192470276')\n",
      "('Epoch: ', '04', ', Loss=', '0.540464895')\n",
      "('Epoch: ', '05', ', Loss=', '0.388268061', ', Validation accuracy=', 0.9034)\n",
      "('Epoch: ', '06', ', Loss=', '0.319350633')\n",
      "('Epoch: ', '07', ', Loss=', '0.276773714')\n",
      "('Epoch: ', '08', ', Loss=', '0.248803798')\n",
      "('Epoch: ', '09', ', Loss=', '0.227074768')\n",
      "('Epoch: ', '10', ', Loss=', '0.209777506', ', Validation accuracy=', 0.94739997)\n",
      "('Epoch: ', '11', ', Loss=', '0.195845701')\n",
      "('Epoch: ', '12', ', Loss=', '0.183973548')\n",
      "('Epoch: ', '13', ', Loss=', '0.173336949')\n",
      "('Epoch: ', '14', ', Loss=', '0.164755849')\n",
      "('Epoch: ', '15', ', Loss=', '0.157564699', ', Validation accuracy=', 0.9612)\n",
      "('Epoch: ', '16', ', Loss=', '0.150178961')\n",
      "('Epoch: ', '17', ', Loss=', '0.143786048')\n",
      "('Epoch: ', '18', ', Loss=', '0.138423375')\n",
      "('Epoch: ', '19', ', Loss=', '0.133648101')\n",
      "('Epoch: ', '20', ', Loss=', '0.128578850', ', Validation accuracy=', 0.96619999)\n",
      "('Epoch: ', '21', ', Loss=', '0.124752569')\n",
      "('Epoch: ', '22', ', Loss=', '0.120349665')\n",
      "('Epoch: ', '23', ', Loss=', '0.117350300')\n",
      "('Epoch: ', '24', ', Loss=', '0.113998679')\n",
      "('Epoch: ', '25', ', Loss=', '0.111119155', ', Validation accuracy=', 0.97100002)\n",
      "('Epoch: ', '26', ', Loss=', '0.108082007')\n",
      "('Epoch: ', '27', ', Loss=', '0.105806645')\n",
      "('Epoch: ', '28', ', Loss=', '0.103265103')\n",
      "('Epoch: ', '29', ', Loss=', '0.101007827')\n",
      "('Epoch: ', '30', ', Loss=', '0.098675160', ', Validation accuracy=', 0.97240001)\n",
      "('Epoch: ', '31', ', Loss=', '0.096683567')\n",
      "('Epoch: ', '32', ', Loss=', '0.094555679')\n",
      "('Epoch: ', '33', ', Loss=', '0.092899651')\n",
      "('Epoch: ', '34', ', Loss=', '0.091177412')\n",
      "('Epoch: ', '35', ', Loss=', '0.089575213', ', Validation accuracy=', 0.97399998)\n",
      "('Epoch: ', '36', ', Loss=', '0.087826453')\n",
      "('Epoch: ', '37', ', Loss=', '0.086287812')\n",
      "('Epoch: ', '38', ', Loss=', '0.085525528')\n",
      "('Epoch: ', '39', ', Loss=', '0.083448837')\n",
      "('Epoch: ', '40', ', Loss=', '0.082172949', ', Validation accuracy=', 0.977)\n",
      "('Epoch: ', '41', ', Loss=', '0.081229873')\n",
      "('Epoch: ', '42', ', Loss=', '0.079788633')\n",
      "('Epoch: ', '43', ', Loss=', '0.079011170')\n",
      "('Epoch: ', '44', ', Loss=', '0.077980586')\n",
      "('Epoch: ', '45', ', Loss=', '0.076310891', ', Validation accuracy=', 0.97619998)\n",
      "('Epoch: ', '46', ', Loss=', '0.075870468')\n",
      "('Epoch: ', '47', ', Loss=', '0.074538083')\n",
      "('Epoch: ', '48', ', Loss=', '0.073426704')\n",
      "('Epoch: ', '49', ', Loss=', '0.072184785')\n",
      "('Epoch: ', '50', ', Loss=', '0.072048182', ', Validation accuracy=', 0.97860003)\n",
      "('Epoch: ', '51', ', Loss=', '0.070546655')\n",
      "('Epoch: ', '52', ', Loss=', '0.069933682')\n",
      "('Epoch: ', '53', ', Loss=', '0.069316038')\n",
      "('Epoch: ', '54', ', Loss=', '0.068243495')\n",
      "('Epoch: ', '55', ', Loss=', '0.067664013', ', Validation accuracy=', 0.97939998)\n",
      "('Epoch: ', '56', ', Loss=', '0.066216306')\n",
      "('Epoch: ', '57', ', Loss=', '0.065795645')\n",
      "('Epoch: ', '58', ', Loss=', '0.065050293')\n",
      "('Epoch: ', '59', ', Loss=', '0.064411021')\n",
      "('Epoch: ', '60', ', Loss=', '0.063615841', ', Validation accuracy=', 0.9788)\n",
      "('Epoch: ', '61', ', Loss=', '0.063362320')\n",
      "('Epoch: ', '62', ', Loss=', '0.062326996')\n",
      "('Epoch: ', '63', ', Loss=', '0.062109343')\n",
      "('Epoch: ', '64', ', Loss=', '0.061012798')\n",
      "('Epoch: ', '65', ', Loss=', '0.060638860', ', Validation accuracy=', 0.98079997)\n",
      "('Epoch: ', '66', ', Loss=', '0.059918800')\n",
      "('Epoch: ', '67', ', Loss=', '0.059358843')\n",
      "('Epoch: ', '68', ', Loss=', '0.058809187')\n",
      "('Epoch: ', '69', ', Loss=', '0.058374522')\n",
      "('Epoch: ', '70', ', Loss=', '0.057479041', ', Validation accuracy=', 0.98079997)\n",
      "('Epoch: ', '71', ', Loss=', '0.057257307')\n",
      "('Epoch: ', '72', ', Loss=', '0.056334803')\n",
      "('Epoch: ', '73', ', Loss=', '0.056265054')\n",
      "('Epoch: ', '74', ', Loss=', '0.055656843')\n",
      "('Epoch: ', '75', ', Loss=', '0.055008239', ', Validation accuracy=', 0.97960001)\n",
      "('Epoch: ', '76', ', Loss=', '0.054559738')\n",
      "('Epoch: ', '77', ', Loss=', '0.054616589')\n",
      "('Epoch: ', '78', ', Loss=', '0.053448106')\n",
      "('Epoch: ', '79', ', Loss=', '0.053130765')\n",
      "('Epoch: ', '80', ', Loss=', '0.053023648', ', Validation accuracy=', 0.98119998)\n",
      "('Epoch: ', '81', ', Loss=', '0.052367939')\n",
      "('Epoch: ', '82', ', Loss=', '0.051924095')\n",
      "('Epoch: ', '83', ', Loss=', '0.051264458')\n",
      "('Epoch: ', '84', ', Loss=', '0.050750131')\n",
      "('Epoch: ', '85', ', Loss=', '0.050238623', ', Validation accuracy=', 0.98199999)\n",
      "('Epoch: ', '86', ', Loss=', '0.050101765')\n",
      "('Epoch: ', '87', ', Loss=', '0.049771321')\n",
      "('Epoch: ', '88', ', Loss=', '0.049074615')\n",
      "('Epoch: ', '89', ', Loss=', '0.049101176')\n",
      "('Epoch: ', '90', ', Loss=', '0.048407628', ', Validation accuracy=', 0.98199999)\n",
      "('Epoch: ', '91', ', Loss=', '0.047849365')\n",
      "('Epoch: ', '92', ', Loss=', '0.048003047')\n",
      "('Epoch: ', '93', ', Loss=', '0.047604364')\n",
      "('Epoch: ', '94', ', Loss=', '0.046771158')\n",
      "('Epoch: ', '95', ', Loss=', '0.046581285', ', Validation accuracy=', 0.97979999)\n",
      "('Epoch: ', '96', ', Loss=', '0.046020882')\n",
      "('Epoch: ', '97', ', Loss=', '0.045941917')\n",
      "('Epoch: ', '98', ', Loss=', '0.045201871')\n",
      "('Epoch: ', '99', ', Loss=', '0.044888001')\n",
      "('Epoch: ', '100', ', Loss=', '0.044877991', ', Validation accuracy=', 0.9824)\n",
      "Optimization Finished! in 5389.948667 seconds\n",
      "('Accuracy on test data: ', [0.98299998])\n",
      "****************************************************************************************************\n",
      "LR=0.0001 \t- Batchsize =50 \t - GradientDescent\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '2.311725026')\n",
      "('Epoch: ', '02', ', Loss=', '2.304043704')\n",
      "('Epoch: ', '03', ', Loss=', '2.296973089')\n",
      "('Epoch: ', '04', ', Loss=', '2.290177902')\n",
      "('Epoch: ', '05', ', Loss=', '2.283378800', ', Validation accuracy=', 0.1318)\n",
      "('Epoch: ', '06', ', Loss=', '2.276263656')\n",
      "('Epoch: ', '07', ', Loss=', '2.268570019')\n",
      "('Epoch: ', '08', ', Loss=', '2.260049698')\n",
      "('Epoch: ', '09', ', Loss=', '2.250460631')\n",
      "('Epoch: ', '10', ', Loss=', '2.239527105', ', Validation accuracy=', 0.2374)\n",
      "('Epoch: ', '11', ', Loss=', '2.226869936')\n",
      "('Epoch: ', '12', ', Loss=', '2.211983925')\n",
      "('Epoch: ', '13', ', Loss=', '2.194185149')\n",
      "('Epoch: ', '14', ', Loss=', '2.172592112')\n",
      "('Epoch: ', '15', ', Loss=', '2.145994016', ', Validation accuracy=', 0.4316)\n",
      "('Epoch: ', '16', ', Loss=', '2.112734365')\n",
      "('Epoch: ', '17', ', Loss=', '2.070692003')\n",
      "('Epoch: ', '18', ', Loss=', '2.017177442')\n",
      "('Epoch: ', '19', ', Loss=', '1.949095004')\n",
      "('Epoch: ', '20', ', Loss=', '1.863127827', ', Validation accuracy=', 0.56739998)\n",
      "('Epoch: ', '21', ', Loss=', '1.756513937')\n",
      "('Epoch: ', '22', ', Loss=', '1.628938733')\n",
      "('Epoch: ', '23', ', Loss=', '1.484395398')\n",
      "('Epoch: ', '24', ', Loss=', '1.330822257')\n",
      "('Epoch: ', '25', ', Loss=', '1.179217056', ', Validation accuracy=', 0.7482)\n",
      "('Epoch: ', '26', ', Loss=', '1.040411030')\n",
      "('Epoch: ', '27', ', Loss=', '0.921135379')\n",
      "('Epoch: ', '28', ', Loss=', '0.823607918')\n",
      "('Epoch: ', '29', ', Loss=', '0.745788391')\n",
      "('Epoch: ', '30', ', Loss=', '0.683729166', ', Validation accuracy=', 0.82980001)\n",
      "('Epoch: ', '31', ', Loss=', '0.633724923')\n",
      "('Epoch: ', '32', ', Loss=', '0.592916222')\n",
      "('Epoch: ', '33', ', Loss=', '0.559083226')\n",
      "('Epoch: ', '34', ', Loss=', '0.530531467')\n",
      "('Epoch: ', '35', ', Loss=', '0.506125559', ', Validation accuracy=', 0.86760002)\n",
      "('Epoch: ', '36', ', Loss=', '0.484998824')\n",
      "('Epoch: ', '37', ', Loss=', '0.466536368')\n",
      "('Epoch: ', '38', ', Loss=', '0.450265879')\n",
      "('Epoch: ', '39', ', Loss=', '0.435664040')\n",
      "('Epoch: ', '40', ', Loss=', '0.422711313', ', Validation accuracy=', 0.88819999)\n",
      "('Epoch: ', '41', ', Loss=', '0.410897729')\n",
      "('Epoch: ', '42', ', Loss=', '0.400228248')\n",
      "('Epoch: ', '43', ', Loss=', '0.390562919')\n",
      "('Epoch: ', '44', ', Loss=', '0.381685241')\n",
      "('Epoch: ', '45', ', Loss=', '0.373400319', ', Validation accuracy=', 0.8976)\n",
      "('Epoch: ', '46', ', Loss=', '0.365732965')\n",
      "('Epoch: ', '47', ', Loss=', '0.358656504')\n",
      "('Epoch: ', '48', ', Loss=', '0.352003512')\n",
      "('Epoch: ', '49', ', Loss=', '0.345781934')\n",
      "('Epoch: ', '50', ', Loss=', '0.339871286', ', Validation accuracy=', 0.90740001)\n",
      "('Epoch: ', '51', ', Loss=', '0.334296833')\n",
      "('Epoch: ', '52', ', Loss=', '0.329019173')\n",
      "('Epoch: ', '53', ', Loss=', '0.324024735')\n",
      "('Epoch: ', '54', ', Loss=', '0.319275330')\n",
      "('Epoch: ', '55', ', Loss=', '0.314771221', ', Validation accuracy=', 0.91420001)\n",
      "('Epoch: ', '56', ', Loss=', '0.310488836')\n",
      "('Epoch: ', '57', ', Loss=', '0.306196679')\n",
      "('Epoch: ', '58', ', Loss=', '0.302315141')\n",
      "('Epoch: ', '59', ', Loss=', '0.298615422')\n",
      "('Epoch: ', '60', ', Loss=', '0.294915376', ', Validation accuracy=', 0.91799998)\n",
      "('Epoch: ', '61', ', Loss=', '0.291434462')\n",
      "('Epoch: ', '62', ', Loss=', '0.288124532')\n",
      "('Epoch: ', '63', ', Loss=', '0.284979287')\n",
      "('Epoch: ', '64', ', Loss=', '0.281851887')\n",
      "('Epoch: ', '65', ', Loss=', '0.278881563', ', Validation accuracy=', 0.9224)\n",
      "('Epoch: ', '66', ', Loss=', '0.276040285')\n",
      "('Epoch: ', '67', ', Loss=', '0.273187221')\n",
      "('Epoch: ', '68', ', Loss=', '0.270452233')\n",
      "('Epoch: ', '69', ', Loss=', '0.267860696')\n",
      "('Epoch: ', '70', ', Loss=', '0.265165997', ', Validation accuracy=', 0.926)\n",
      "('Epoch: ', '71', ', Loss=', '0.262719268')\n",
      "('Epoch: ', '72', ', Loss=', '0.260228812')\n",
      "('Epoch: ', '73', ', Loss=', '0.257934994')\n",
      "('Epoch: ', '74', ', Loss=', '0.255630208')\n",
      "('Epoch: ', '75', ', Loss=', '0.253308978', ', Validation accuracy=', 0.93120003)\n",
      "('Epoch: ', '76', ', Loss=', '0.251102193')\n",
      "('Epoch: ', '77', ', Loss=', '0.248985731')\n",
      "('Epoch: ', '78', ', Loss=', '0.246807377')\n",
      "('Epoch: ', '79', ', Loss=', '0.244818373')\n",
      "('Epoch: ', '80', ', Loss=', '0.242705394', ', Validation accuracy=', 0.93260002)\n",
      "('Epoch: ', '81', ', Loss=', '0.240731859')\n",
      "('Epoch: ', '82', ', Loss=', '0.238787596')\n",
      "('Epoch: ', '83', ', Loss=', '0.236839103')\n",
      "('Epoch: ', '84', ', Loss=', '0.234979578')\n",
      "('Epoch: ', '85', ', Loss=', '0.233128991', ', Validation accuracy=', 0.93480003)\n",
      "('Epoch: ', '86', ', Loss=', '0.231387463')\n",
      "('Epoch: ', '87', ', Loss=', '0.229608005')\n",
      "('Epoch: ', '88', ', Loss=', '0.227884886')\n",
      "('Epoch: ', '89', ', Loss=', '0.226059790')\n",
      "('Epoch: ', '90', ', Loss=', '0.224453776', ', Validation accuracy=', 0.93900001)\n",
      "('Epoch: ', '91', ', Loss=', '0.222826110')\n",
      "('Epoch: ', '92', ', Loss=', '0.221052607')\n",
      "('Epoch: ', '93', ', Loss=', '0.219673797')\n",
      "('Epoch: ', '94', ', Loss=', '0.217953655')\n",
      "('Epoch: ', '95', ', Loss=', '0.216432519', ', Validation accuracy=', 0.94139999)\n",
      "('Epoch: ', '96', ', Loss=', '0.214930030')\n",
      "('Epoch: ', '97', ', Loss=', '0.213459712')\n",
      "('Epoch: ', '98', ', Loss=', '0.212017420')\n",
      "('Epoch: ', '99', ', Loss=', '0.210500741')\n",
      "('Epoch: ', '100', ', Loss=', '0.209110503', ', Validation accuracy=', 0.94419998)\n",
      "Optimization Finished! in 4369.491698 seconds\n",
      "('Accuracy on test data: ', [0.94330001])\n"
     ]
    }
   ],
   "source": [
    "# your answer goas here\n",
    "print '*'*100\n",
    "print 'LR=0.001 \\t- Batchsize =50 \\t - GradientDescent'\n",
    "print '*'*100\n",
    "mnist_relu_001_50 = NeuralNetwork(LR=0.001, Iter=100, Batchsize=50, display_step= 5 ,\n",
    "                         Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.relu, NN_Name=\"\")\n",
    "mnist_relu_001_50.train(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "print '*'*100\n",
    "print 'LR=0.0001 \\t- Batchsize =50 \\t - GradientDescent'\n",
    "print '*'*100\n",
    "mnist_relu_0001_50 = NeuralNetwork(LR=0.0001, Iter=100, Batchsize=50, display_step= 5 ,\n",
    "                         Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_0001_50.train(X_train, y_train, X_validation, y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "LR=0.001 \t- Batchsize =128 \t - GradientDescent\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '2.293662045')\n",
      "('Epoch: ', '02', ', Loss=', '2.263085521')\n",
      "('Epoch: ', '03', ', Loss=', '2.210614881')\n",
      "('Epoch: ', '04', ', Loss=', '2.086808465')\n",
      "('Epoch: ', '05', ', Loss=', '1.732091003', ', Validation accuracy=', 0.6846)\n",
      "('Epoch: ', '06', ', Loss=', '1.114975541')\n",
      "('Epoch: ', '07', ', Loss=', '0.724923310')\n",
      "('Epoch: ', '08', ', Loss=', '0.558699357')\n",
      "('Epoch: ', '09', ', Loss=', '0.471472615')\n",
      "('Epoch: ', '10', ', Loss=', '0.417749414', ', Validation accuracy=', 0.89359999)\n",
      "('Epoch: ', '11', ', Loss=', '0.380432177')\n",
      "('Epoch: ', '12', ', Loss=', '0.353277691')\n",
      "('Epoch: ', '13', ', Loss=', '0.331919414')\n",
      "('Epoch: ', '14', ', Loss=', '0.314109981')\n",
      "('Epoch: ', '15', ', Loss=', '0.298804025', ', Validation accuracy=', 0.92040002)\n",
      "('Epoch: ', '16', ', Loss=', '0.285288136')\n",
      "('Epoch: ', '17', ', Loss=', '0.273801311')\n",
      "('Epoch: ', '18', ', Loss=', '0.263017541')\n",
      "('Epoch: ', '19', ', Loss=', '0.253475492')\n",
      "('Epoch: ', '20', ', Loss=', '0.244738529', ', Validation accuracy=', 0.93300003)\n",
      "('Epoch: ', '21', ', Loss=', '0.236742698')\n",
      "('Epoch: ', '22', ', Loss=', '0.229089423')\n",
      "('Epoch: ', '23', ', Loss=', '0.222047315')\n",
      "('Epoch: ', '24', ', Loss=', '0.216245183')\n",
      "('Epoch: ', '25', ', Loss=', '0.209722039', ', Validation accuracy=', 0.94300002)\n",
      "('Epoch: ', '26', ', Loss=', '0.203957884')\n",
      "('Epoch: ', '27', ', Loss=', '0.198917164')\n",
      "('Epoch: ', '28', ', Loss=', '0.193709644')\n",
      "('Epoch: ', '29', ', Loss=', '0.189058459')\n",
      "('Epoch: ', '30', ', Loss=', '0.184415746', ', Validation accuracy=', 0.95020002)\n",
      "('Epoch: ', '31', ', Loss=', '0.180265293')\n",
      "('Epoch: ', '32', ', Loss=', '0.176294522')\n",
      "('Epoch: ', '33', ', Loss=', '0.172181047')\n",
      "('Epoch: ', '34', ', Loss=', '0.168927643')\n",
      "('Epoch: ', '35', ', Loss=', '0.165365691', ', Validation accuracy=', 0.95380002)\n",
      "('Epoch: ', '36', ', Loss=', '0.162148459')\n",
      "('Epoch: ', '37', ', Loss=', '0.158882936')\n",
      "('Epoch: ', '38', ', Loss=', '0.155902371')\n",
      "('Epoch: ', '39', ', Loss=', '0.153161537')\n",
      "('Epoch: ', '40', ', Loss=', '0.150442951', ', Validation accuracy=', 0.95880002)\n",
      "('Epoch: ', '41', ', Loss=', '0.147537440')\n",
      "('Epoch: ', '42', ', Loss=', '0.145125171')\n",
      "('Epoch: ', '43', ', Loss=', '0.142599480')\n",
      "('Epoch: ', '44', ', Loss=', '0.140342388')\n",
      "('Epoch: ', '45', ', Loss=', '0.138141952', ', Validation accuracy=', 0.96380001)\n",
      "('Epoch: ', '46', ', Loss=', '0.136016730')\n",
      "('Epoch: ', '47', ', Loss=', '0.134096873')\n",
      "('Epoch: ', '48', ', Loss=', '0.132135548')\n",
      "('Epoch: ', '49', ', Loss=', '0.130017343')\n",
      "('Epoch: ', '50', ', Loss=', '0.128385124', ', Validation accuracy=', 0.96700001)\n",
      "('Epoch: ', '51', ', Loss=', '0.126796425')\n",
      "('Epoch: ', '52', ', Loss=', '0.125006025')\n",
      "('Epoch: ', '53', ', Loss=', '0.123403239')\n",
      "('Epoch: ', '54', ', Loss=', '0.122010727')\n",
      "('Epoch: ', '55', ', Loss=', '0.120435495', ', Validation accuracy=', 0.96799999)\n",
      "('Epoch: ', '56', ', Loss=', '0.118756036')\n",
      "('Epoch: ', '57', ', Loss=', '0.117302557')\n",
      "('Epoch: ', '58', ', Loss=', '0.116140501')\n",
      "('Epoch: ', '59', ', Loss=', '0.114646131')\n",
      "('Epoch: ', '60', ', Loss=', '0.113435144', ', Validation accuracy=', 0.96899998)\n",
      "('Epoch: ', '61', ', Loss=', '0.112170942')\n",
      "('Epoch: ', '62', ', Loss=', '0.110964983')\n",
      "('Epoch: ', '63', ', Loss=', '0.109854003')\n",
      "('Epoch: ', '64', ', Loss=', '0.108869269')\n",
      "('Epoch: ', '65', ', Loss=', '0.107803020', ', Validation accuracy=', 0.9716)\n",
      "('Epoch: ', '66', ', Loss=', '0.106557818')\n",
      "('Epoch: ', '67', ', Loss=', '0.105466891')\n",
      "('Epoch: ', '68', ', Loss=', '0.104495576')\n",
      "('Epoch: ', '69', ', Loss=', '0.103426324')\n",
      "('Epoch: ', '70', ', Loss=', '0.102593067', ', Validation accuracy=', 0.97119999)\n",
      "('Epoch: ', '71', ', Loss=', '0.101399699')\n",
      "('Epoch: ', '72', ', Loss=', '0.100555334')\n",
      "('Epoch: ', '73', ', Loss=', '0.099786376')\n",
      "('Epoch: ', '74', ', Loss=', '0.098958706')\n",
      "('Epoch: ', '75', ', Loss=', '0.097981878', ', Validation accuracy=', 0.9716)\n",
      "('Epoch: ', '76', ', Loss=', '0.097266059')\n",
      "('Epoch: ', '77', ', Loss=', '0.096473819')\n",
      "('Epoch: ', '78', ', Loss=', '0.095684234')\n",
      "('Epoch: ', '79', ', Loss=', '0.094673459')\n",
      "('Epoch: ', '80', ', Loss=', '0.094050569', ', Validation accuracy=', 0.97219998)\n",
      "('Epoch: ', '81', ', Loss=', '0.093287464')\n",
      "('Epoch: ', '82', ', Loss=', '0.092693057')\n",
      "('Epoch: ', '83', ', Loss=', '0.091823553')\n",
      "('Epoch: ', '84', ', Loss=', '0.091114495')\n",
      "('Epoch: ', '85', ', Loss=', '0.090518905', ', Validation accuracy=', 0.97280002)\n",
      "('Epoch: ', '86', ', Loss=', '0.089755357')\n",
      "('Epoch: ', '87', ', Loss=', '0.089184875')\n",
      "('Epoch: ', '88', ', Loss=', '0.088555112')\n",
      "('Epoch: ', '89', ', Loss=', '0.087736129')\n",
      "('Epoch: ', '90', ', Loss=', '0.087283453', ', Validation accuracy=', 0.97420001)\n",
      "('Epoch: ', '91', ', Loss=', '0.086870731')\n",
      "('Epoch: ', '92', ', Loss=', '0.085932059')\n",
      "('Epoch: ', '93', ', Loss=', '0.085365588')\n",
      "('Epoch: ', '94', ', Loss=', '0.084847923')\n",
      "('Epoch: ', '95', ', Loss=', '0.084282373', ', Validation accuracy=', 0.97439998)\n",
      "('Epoch: ', '96', ', Loss=', '0.083712187')\n",
      "('Epoch: ', '97', ', Loss=', '0.083004646')\n",
      "('Epoch: ', '98', ', Loss=', '0.082643872')\n",
      "('Epoch: ', '99', ', Loss=', '0.082076117')\n",
      "('Epoch: ', '100', ', Loss=', '0.081515838', ', Validation accuracy=', 0.97439998)\n",
      "Optimization Finished! in 2437.316396 seconds\n",
      "('Accuracy on test data: ', [0.97750002])\n",
      "****************************************************************************************************\n",
      "LR=0.0001 \t- Batchsize =128 \t - GradientDescent\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '2.323540621')\n",
      "('Epoch: ', '02', ', Loss=', '2.319346102')\n",
      "('Epoch: ', '03', ', Loss=', '2.315473722')\n",
      "('Epoch: ', '04', ', Loss=', '2.311870468')\n",
      "('Epoch: ', '05', ', Loss=', '2.308504926', ', Validation accuracy=', 0.048799999)\n",
      "('Epoch: ', '06', ', Loss=', '2.305336210')\n",
      "('Epoch: ', '07', ', Loss=', '2.302296423')\n",
      "('Epoch: ', '08', ', Loss=', '2.299386092')\n",
      "('Epoch: ', '09', ', Loss=', '2.296548045')\n",
      "('Epoch: ', '10', ', Loss=', '2.293784941', ', Validation accuracy=', 0.068000004)\n",
      "('Epoch: ', '11', ', Loss=', '2.291018760')\n",
      "('Epoch: ', '12', ', Loss=', '2.288275424')\n",
      "('Epoch: ', '13', ', Loss=', '2.285530488')\n",
      "('Epoch: ', '14', ', Loss=', '2.282764992')\n",
      "('Epoch: ', '15', ', Loss=', '2.279953486', ', Validation accuracy=', 0.092200004)\n",
      "('Epoch: ', '16', ', Loss=', '2.277082065')\n",
      "('Epoch: ', '17', ', Loss=', '2.274132046')\n",
      "('Epoch: ', '18', ', Loss=', '2.271068321')\n",
      "('Epoch: ', '19', ', Loss=', '2.267899314')\n",
      "('Epoch: ', '20', ', Loss=', '2.264593708', ', Validation accuracy=', 0.1222)\n",
      "('Epoch: ', '21', ', Loss=', '2.261135702')\n",
      "('Epoch: ', '22', ', Loss=', '2.257510019')\n",
      "('Epoch: ', '23', ', Loss=', '2.253668060')\n",
      "('Epoch: ', '24', ', Loss=', '2.249589706')\n",
      "('Epoch: ', '25', ', Loss=', '2.245273217', ', Validation accuracy=', 0.1758)\n",
      "('Epoch: ', '26', ', Loss=', '2.240676977')\n",
      "('Epoch: ', '27', ', Loss=', '2.235773808')\n",
      "('Epoch: ', '28', ', Loss=', '2.230539392')\n",
      "('Epoch: ', '29', ', Loss=', '2.224944379')\n",
      "('Epoch: ', '30', ', Loss=', '2.218914694', ', Validation accuracy=', 0.24680001)\n",
      "('Epoch: ', '31', ', Loss=', '2.212461540')\n",
      "('Epoch: ', '32', ', Loss=', '2.205495554')\n",
      "('Epoch: ', '33', ', Loss=', '2.198033466')\n",
      "('Epoch: ', '34', ', Loss=', '2.189990231')\n",
      "('Epoch: ', '35', ', Loss=', '2.181306114', ', Validation accuracy=', 0.34099999)\n",
      "('Epoch: ', '36', ', Loss=', '2.171946184')\n",
      "('Epoch: ', '37', ', Loss=', '2.161770928')\n",
      "('Epoch: ', '38', ', Loss=', '2.150692188')\n",
      "('Epoch: ', '39', ', Loss=', '2.138663573')\n",
      "('Epoch: ', '40', ', Loss=', '2.125469193', ', Validation accuracy=', 0.43959999)\n",
      "('Epoch: ', '41', ', Loss=', '2.111047484')\n",
      "('Epoch: ', '42', ', Loss=', '2.095226978')\n",
      "('Epoch: ', '43', ', Loss=', '2.077832184')\n",
      "('Epoch: ', '44', ', Loss=', '2.058689517')\n",
      "('Epoch: ', '45', ', Loss=', '2.037537421', ', Validation accuracy=', 0.52780002)\n",
      "('Epoch: ', '46', ', Loss=', '2.014235176')\n",
      "('Epoch: ', '47', ', Loss=', '1.988539107')\n",
      "('Epoch: ', '48', ', Loss=', '1.960284135')\n",
      "('Epoch: ', '49', ', Loss=', '1.929173719')\n",
      "('Epoch: ', '50', ', Loss=', '1.895202493', ', Validation accuracy=', 0.59579998)\n",
      "('Epoch: ', '51', ', Loss=', '1.858014170')\n",
      "('Epoch: ', '52', ', Loss=', '1.817727049')\n",
      "('Epoch: ', '53', ', Loss=', '1.773983541')\n",
      "('Epoch: ', '54', ', Loss=', '1.727047869')\n",
      "('Epoch: ', '55', ', Loss=', '1.677003515', ', Validation accuracy=', 0.64859998)\n",
      "('Epoch: ', '56', ', Loss=', '1.624222870')\n",
      "('Epoch: ', '57', ', Loss=', '1.569444496')\n",
      "('Epoch: ', '58', ', Loss=', '1.513153167')\n",
      "('Epoch: ', '59', ', Loss=', '1.456154951')\n",
      "('Epoch: ', '60', ', Loss=', '1.398938424', ', Validation accuracy=', 0.69940001)\n",
      "('Epoch: ', '61', ', Loss=', '1.342227814')\n",
      "('Epoch: ', '62', ', Loss=', '1.286421191')\n",
      "('Epoch: ', '63', ', Loss=', '1.232003361')\n",
      "('Epoch: ', '64', ', Loss=', '1.179394099')\n",
      "('Epoch: ', '65', ', Loss=', '1.129031275', ', Validation accuracy=', 0.74059999)\n",
      "('Epoch: ', '66', ', Loss=', '1.081067934')\n",
      "('Epoch: ', '67', ', Loss=', '1.035656960')\n",
      "('Epoch: ', '68', ', Loss=', '0.992857420')\n",
      "('Epoch: ', '69', ', Loss=', '0.952783003')\n",
      "('Epoch: ', '70', ', Loss=', '0.915484443', ', Validation accuracy=', 0.77880001)\n",
      "('Epoch: ', '71', ', Loss=', '0.881252098')\n",
      "('Epoch: ', '72', ', Loss=', '0.849719776')\n",
      "('Epoch: ', '73', ', Loss=', '0.820749455')\n",
      "('Epoch: ', '74', ', Loss=', '0.794241437')\n",
      "('Epoch: ', '75', ', Loss=', '0.769694256', ', Validation accuracy=', 0.80860001)\n",
      "('Epoch: ', '76', ', Loss=', '0.747277908')\n",
      "('Epoch: ', '77', ', Loss=', '0.726502279')\n",
      "('Epoch: ', '78', ', Loss=', '0.707369038')\n",
      "('Epoch: ', '79', ', Loss=', '0.689373515')\n",
      "('Epoch: ', '80', ', Loss=', '0.672875641', ', Validation accuracy=', 0.8272)\n",
      "('Epoch: ', '81', ', Loss=', '0.657266995')\n",
      "('Epoch: ', '82', ', Loss=', '0.642922840')\n",
      "('Epoch: ', '83', ', Loss=', '0.629251768')\n",
      "('Epoch: ', '84', ', Loss=', '0.616486532')\n",
      "('Epoch: ', '85', ', Loss=', '0.604401954', ', Validation accuracy=', 0.83960003)\n",
      "('Epoch: ', '86', ', Loss=', '0.592870013')\n",
      "('Epoch: ', '87', ', Loss=', '0.582024674')\n",
      "('Epoch: ', '88', ', Loss=', '0.571842719')\n",
      "('Epoch: ', '89', ', Loss=', '0.562109441')\n",
      "('Epoch: ', '90', ', Loss=', '0.553013554', ', Validation accuracy=', 0.85299999)\n",
      "('Epoch: ', '91', ', Loss=', '0.544153426')\n",
      "('Epoch: ', '92', ', Loss=', '0.535846577')\n",
      "('Epoch: ', '93', ', Loss=', '0.527820139')\n",
      "('Epoch: ', '94', ', Loss=', '0.520279456')\n",
      "('Epoch: ', '95', ', Loss=', '0.513075739', ', Validation accuracy=', 0.86360002)\n",
      "('Epoch: ', '96', ', Loss=', '0.506216908')\n",
      "('Epoch: ', '97', ', Loss=', '0.499438724')\n",
      "('Epoch: ', '98', ', Loss=', '0.493218676')\n",
      "('Epoch: ', '99', ', Loss=', '0.487033639')\n",
      "('Epoch: ', '100', ', Loss=', '0.481232127', ', Validation accuracy=', 0.87199998)\n",
      "Optimization Finished! in 2170.266137 seconds\n",
      "('Accuracy on test data: ', [0.87019998])\n",
      "****************************************************************************************************\n",
      "LR=0.001 \t- Batchsize =50 \t - AdamOptimizer\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '0.224882553')\n",
      "('Epoch: ', '02', ', Loss=', '0.065745450')\n",
      "('Epoch: ', '03', ', Loss=', '0.046186085')\n",
      "('Epoch: ', '04', ', Loss=', '0.035051029')\n",
      "('Epoch: ', '05', ', Loss=', '0.029580894', ', Validation accuracy=', 0.98860002)\n",
      "('Epoch: ', '06', ', Loss=', '0.022925862')\n",
      "('Epoch: ', '07', ', Loss=', '0.021130482')\n",
      "('Epoch: ', '08', ', Loss=', '0.016403644')\n",
      "('Epoch: ', '09', ', Loss=', '0.014497017')\n",
      "('Epoch: ', '10', ', Loss=', '0.013095707', ', Validation accuracy=', 0.99239999)\n",
      "('Epoch: ', '11', ', Loss=', '0.012719569')\n",
      "('Epoch: ', '12', ', Loss=', '0.009319548')\n",
      "('Epoch: ', '13', ', Loss=', '0.012057845')\n",
      "('Epoch: ', '14', ', Loss=', '0.007481272')\n",
      "('Epoch: ', '15', ', Loss=', '0.007427864', ', Validation accuracy=', 0.98379999)\n",
      "('Epoch: ', '16', ', Loss=', '0.009492599')\n",
      "('Epoch: ', '17', ', Loss=', '0.007204340')\n",
      "('Epoch: ', '18', ', Loss=', '0.005285176')\n",
      "('Epoch: ', '19', ', Loss=', '0.007500075')\n",
      "('Epoch: ', '20', ', Loss=', '0.005702758', ', Validation accuracy=', 0.99159998)\n",
      "('Epoch: ', '21', ', Loss=', '0.004672305')\n",
      "('Epoch: ', '22', ', Loss=', '0.005025886')\n",
      "('Epoch: ', '23', ', Loss=', '0.007045448')\n",
      "('Epoch: ', '24', ', Loss=', '0.004084627')\n",
      "('Epoch: ', '25', ', Loss=', '0.006315826', ', Validation accuracy=', 0.98879999)\n",
      "('Epoch: ', '26', ', Loss=', '0.004448865')\n",
      "('Epoch: ', '27', ', Loss=', '0.004864812')\n",
      "('Epoch: ', '28', ', Loss=', '0.006411770')\n",
      "('Epoch: ', '29', ', Loss=', '0.003158979')\n",
      "('Epoch: ', '30', ', Loss=', '0.005663160', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '31', ', Loss=', '0.002891146')\n",
      "('Epoch: ', '32', ', Loss=', '0.004482584')\n",
      "('Epoch: ', '33', ', Loss=', '0.003036166')\n",
      "('Epoch: ', '34', ', Loss=', '0.004738358')\n",
      "('Epoch: ', '35', ', Loss=', '0.002719123', ', Validation accuracy=', 0.9878)\n",
      "('Epoch: ', '36', ', Loss=', '0.005829441')\n",
      "('Epoch: ', '37', ', Loss=', '0.002886494')\n",
      "('Epoch: ', '38', ', Loss=', '0.003364252')\n",
      "('Epoch: ', '39', ', Loss=', '0.004997699')\n",
      "('Epoch: ', '40', ', Loss=', '0.004302843', ', Validation accuracy=', 0.98900002)\n",
      "('Epoch: ', '41', ', Loss=', '0.002007124')\n",
      "('Epoch: ', '42', ', Loss=', '0.004811551')\n",
      "('Epoch: ', '43', ', Loss=', '0.002409912')\n",
      "('Epoch: ', '44', ', Loss=', '0.001414796')\n",
      "('Epoch: ', '45', ', Loss=', '0.006192742', ', Validation accuracy=', 0.98879999)\n",
      "('Epoch: ', '46', ', Loss=', '0.002029651')\n",
      "('Epoch: ', '47', ', Loss=', '0.003322742')\n",
      "('Epoch: ', '48', ', Loss=', '0.003655319')\n",
      "('Epoch: ', '49', ', Loss=', '0.004322579')\n",
      "('Epoch: ', '50', ', Loss=', '0.001967654', ', Validation accuracy=', 0.99000001)\n",
      "('Epoch: ', '51', ', Loss=', '0.002457901')\n",
      "('Epoch: ', '52', ', Loss=', '0.004802626')\n",
      "('Epoch: ', '53', ', Loss=', '0.004773803')\n",
      "('Epoch: ', '54', ', Loss=', '0.002197815')\n",
      "('Epoch: ', '55', ', Loss=', '0.002883573', ', Validation accuracy=', 0.9896)\n",
      "('Epoch: ', '56', ', Loss=', '0.003951170')\n",
      "('Epoch: ', '57', ', Loss=', '0.003846695')\n",
      "('Epoch: ', '58', ', Loss=', '0.002687929')\n",
      "('Epoch: ', '59', ', Loss=', '0.003477305')\n",
      "('Epoch: ', '60', ', Loss=', '0.002837806', ', Validation accuracy=', 0.99000001)\n",
      "('Epoch: ', '61', ', Loss=', '0.002013162')\n",
      "('Epoch: ', '62', ', Loss=', '0.003590057')\n",
      "('Epoch: ', '63', ', Loss=', '0.002961539')\n",
      "('Epoch: ', '64', ', Loss=', '0.002050367')\n",
      "('Epoch: ', '65', ', Loss=', '0.004632813', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '66', ', Loss=', '0.003644782')\n",
      "('Epoch: ', '67', ', Loss=', '0.003175852')\n",
      "('Epoch: ', '68', ', Loss=', '0.001726378')\n",
      "('Epoch: ', '69', ', Loss=', '0.002792814')\n",
      "('Epoch: ', '70', ', Loss=', '0.004077667', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '71', ', Loss=', '0.002682982')\n",
      "('Epoch: ', '72', ', Loss=', '0.000396432')\n",
      "('Epoch: ', '73', ', Loss=', '0.003114146')\n",
      "('Epoch: ', '74', ', Loss=', '0.004189567')\n",
      "('Epoch: ', '75', ', Loss=', '0.003494565', ', Validation accuracy=', 0.9896)\n",
      "('Epoch: ', '76', ', Loss=', '0.003605664')\n",
      "('Epoch: ', '77', ', Loss=', '0.002285750')\n",
      "('Epoch: ', '78', ', Loss=', '0.003797467')\n",
      "('Epoch: ', '79', ', Loss=', '0.000841201')\n",
      "('Epoch: ', '80', ', Loss=', '0.003632707', ', Validation accuracy=', 0.99080002)\n",
      "('Epoch: ', '81', ', Loss=', '0.003703601')\n",
      "('Epoch: ', '82', ', Loss=', '0.004567641')\n",
      "('Epoch: ', '83', ', Loss=', '0.002089362')\n",
      "('Epoch: ', '84', ', Loss=', '0.000768346')\n",
      "('Epoch: ', '85', ', Loss=', '0.003534206', ', Validation accuracy=', 0.99080002)\n",
      "('Epoch: ', '86', ', Loss=', '0.003789768')\n",
      "('Epoch: ', '87', ', Loss=', '0.002656352')\n",
      "('Epoch: ', '88', ', Loss=', '0.003579771')\n",
      "('Epoch: ', '89', ', Loss=', '0.001342690')\n",
      "('Epoch: ', '90', ', Loss=', '0.004241921', ', Validation accuracy=', 0.99260002)\n",
      "('Epoch: ', '91', ', Loss=', '0.003296940')\n",
      "('Epoch: ', '92', ', Loss=', '0.000745335')\n",
      "('Epoch: ', '93', ', Loss=', '0.000333640')\n",
      "('Epoch: ', '94', ', Loss=', '0.004465579')\n",
      "('Epoch: ', '95', ', Loss=', '0.003518480', ', Validation accuracy=', 0.99199998)\n",
      "('Epoch: ', '96', ', Loss=', '0.003617683')\n",
      "('Epoch: ', '97', ', Loss=', '0.001667135')\n",
      "('Epoch: ', '98', ', Loss=', '0.001771744')\n",
      "('Epoch: ', '99', ', Loss=', '0.005850756')\n",
      "('Epoch: ', '100', ', Loss=', '0.000983789', ', Validation accuracy=', 0.99220002)\n",
      "Optimization Finished! in 15127.756935 seconds\n",
      "('Accuracy on test data: ', [0.99220002])\n",
      "****************************************************************************************************\n",
      "LR=0.0001 \t- Batchsize =50 \t - AdamOptimizer\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '0.752449351')\n",
      "('Epoch: ', '02', ', Loss=', '0.217662309')\n",
      "('Epoch: ', '03', ', Loss=', '0.149019126')\n",
      "('Epoch: ', '04', ', Loss=', '0.115814926')\n",
      "('Epoch: ', '05', ', Loss=', '0.095962128', ', Validation accuracy=', 0.9738)\n",
      "('Epoch: ', '06', ', Loss=', '0.083329739')\n",
      "('Epoch: ', '07', ', Loss=', '0.074200770')\n",
      "('Epoch: ', '08', ', Loss=', '0.067605553')\n",
      "('Epoch: ', '09', ', Loss=', '0.061896066')\n",
      "('Epoch: ', '10', ', Loss=', '0.057194140', ', Validation accuracy=', 0.98299998)\n",
      "('Epoch: ', '11', ', Loss=', '0.052837220')\n",
      "('Epoch: ', '12', ', Loss=', '0.049852332')\n",
      "('Epoch: ', '13', ', Loss=', '0.046925541')\n",
      "('Epoch: ', '14', ', Loss=', '0.043962057')\n",
      "('Epoch: ', '15', ', Loss=', '0.041520053', ', Validation accuracy=', 0.98619998)\n",
      "('Epoch: ', '16', ', Loss=', '0.039039225')\n",
      "('Epoch: ', '17', ', Loss=', '0.037404458')\n",
      "('Epoch: ', '18', ', Loss=', '0.035577465')\n",
      "('Epoch: ', '19', ', Loss=', '0.033332398')\n",
      "('Epoch: ', '20', ', Loss=', '0.032318724', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '21', ', Loss=', '0.029937433')\n",
      "('Epoch: ', '22', ', Loss=', '0.028694517')\n",
      "('Epoch: ', '23', ', Loss=', '0.027480564')\n",
      "('Epoch: ', '24', ', Loss=', '0.026176980')\n",
      "('Epoch: ', '25', ', Loss=', '0.025035054', ', Validation accuracy=', 0.99019998)\n",
      "('Epoch: ', '26', ', Loss=', '0.023465155')\n",
      "('Epoch: ', '27', ', Loss=', '0.022797358')\n",
      "('Epoch: ', '28', ', Loss=', '0.021050088')\n",
      "('Epoch: ', '29', ', Loss=', '0.020495200')\n",
      "('Epoch: ', '30', ', Loss=', '0.019486467', ', Validation accuracy=', 0.98879999)\n",
      "('Epoch: ', '31', ', Loss=', '0.018545263')\n",
      "('Epoch: ', '32', ', Loss=', '0.018594829')\n",
      "('Epoch: ', '33', ', Loss=', '0.016616459')\n",
      "('Epoch: ', '34', ', Loss=', '0.016204984')\n",
      "('Epoch: ', '35', ', Loss=', '0.015284815', ', Validation accuracy=', 0.98900002)\n",
      "('Epoch: ', '36', ', Loss=', '0.014468546')\n",
      "('Epoch: ', '37', ', Loss=', '0.013922409')\n",
      "('Epoch: ', '38', ', Loss=', '0.013177272')\n",
      "('Epoch: ', '39', ', Loss=', '0.012675912')\n",
      "('Epoch: ', '40', ', Loss=', '0.012030213', ', Validation accuracy=', 0.98860002)\n",
      "('Epoch: ', '41', ', Loss=', '0.012098957')\n",
      "('Epoch: ', '42', ', Loss=', '0.010603603')\n",
      "('Epoch: ', '43', ', Loss=', '0.010580083')\n",
      "('Epoch: ', '44', ', Loss=', '0.009757664')\n",
      "('Epoch: ', '45', ', Loss=', '0.009390887', ', Validation accuracy=', 0.9896)\n",
      "('Epoch: ', '46', ', Loss=', '0.008638019')\n",
      "('Epoch: ', '47', ', Loss=', '0.008641191')\n",
      "('Epoch: ', '48', ', Loss=', '0.007508350')\n",
      "('Epoch: ', '49', ', Loss=', '0.008002192')\n",
      "('Epoch: ', '50', ', Loss=', '0.006823707', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '51', ', Loss=', '0.007081603')\n",
      "('Epoch: ', '52', ', Loss=', '0.006720747')\n",
      "('Epoch: ', '53', ', Loss=', '0.006240697')\n",
      "('Epoch: ', '54', ', Loss=', '0.005951844')\n",
      "('Epoch: ', '55', ', Loss=', '0.005433841', ', Validation accuracy=', 0.98540002)\n",
      "('Epoch: ', '56', ', Loss=', '0.006123106')\n",
      "('Epoch: ', '57', ', Loss=', '0.005042612')\n",
      "('Epoch: ', '58', ', Loss=', '0.004672148')\n",
      "('Epoch: ', '59', ', Loss=', '0.004905050')\n",
      "('Epoch: ', '60', ', Loss=', '0.004737564', ', Validation accuracy=', 0.99040002)\n",
      "('Epoch: ', '61', ', Loss=', '0.003783435')\n",
      "('Epoch: ', '62', ', Loss=', '0.004335834')\n",
      "('Epoch: ', '63', ', Loss=', '0.003747673')\n",
      "('Epoch: ', '64', ', Loss=', '0.003764647')\n",
      "('Epoch: ', '65', ', Loss=', '0.003400398', ', Validation accuracy=', 0.98879999)\n",
      "('Epoch: ', '66', ', Loss=', '0.003433843')\n",
      "('Epoch: ', '67', ', Loss=', '0.003078380')\n",
      "('Epoch: ', '68', ', Loss=', '0.003128256')\n",
      "('Epoch: ', '69', ', Loss=', '0.003514061')\n",
      "('Epoch: ', '70', ', Loss=', '0.002116447', ', Validation accuracy=', 0.98979998)\n",
      "('Epoch: ', '71', ', Loss=', '0.003733418')\n",
      "('Epoch: ', '72', ', Loss=', '0.002995134')\n",
      "('Epoch: ', '73', ', Loss=', '0.001633067')\n",
      "('Epoch: ', '74', ', Loss=', '0.002161198')\n",
      "('Epoch: ', '75', ', Loss=', '0.003495518', ', Validation accuracy=', 0.9896)\n",
      "('Epoch: ', '76', ', Loss=', '0.001673867')\n",
      "('Epoch: ', '77', ', Loss=', '0.002497024')\n",
      "('Epoch: ', '78', ', Loss=', '0.002146565')\n",
      "('Epoch: ', '79', ', Loss=', '0.001691600')\n",
      "('Epoch: ', '80', ', Loss=', '0.001981229', ', Validation accuracy=', 0.98699999)\n",
      "('Epoch: ', '81', ', Loss=', '0.002046488')\n",
      "('Epoch: ', '82', ', Loss=', '0.001328227')\n",
      "('Epoch: ', '83', ', Loss=', '0.003094280')\n",
      "('Epoch: ', '84', ', Loss=', '0.001002338')\n",
      "('Epoch: ', '85', ', Loss=', '0.001679442', ', Validation accuracy=', 0.98900002)\n",
      "('Epoch: ', '86', ', Loss=', '0.001931385')\n",
      "('Epoch: ', '87', ', Loss=', '0.001649252')\n",
      "('Epoch: ', '88', ', Loss=', '0.001758796')\n",
      "('Epoch: ', '89', ', Loss=', '0.000985769')\n",
      "('Epoch: ', '90', ', Loss=', '0.002160096', ', Validation accuracy=', 0.98860002)\n",
      "('Epoch: ', '91', ', Loss=', '0.000853591')\n",
      "('Epoch: ', '92', ', Loss=', '0.001793267')\n",
      "('Epoch: ', '93', ', Loss=', '0.000750204')\n",
      "('Epoch: ', '94', ', Loss=', '0.002516135')\n",
      "('Epoch: ', '95', ', Loss=', '0.001603340', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '96', ', Loss=', '0.001214658')\n",
      "('Epoch: ', '97', ', Loss=', '0.001735315')\n",
      "('Epoch: ', '98', ', Loss=', '0.001497581')\n",
      "('Epoch: ', '99', ', Loss=', '0.000325819')\n",
      "('Epoch: ', '100', ', Loss=', '0.000656631', ', Validation accuracy=', 0.98820001)\n",
      "Optimization Finished! in 2441.589484 seconds\n",
      "('Accuracy on test data: ', [0.98790002])\n",
      "****************************************************************************************************\n",
      "LR=0.001 \t- Batchsize =128 \t - AdamOptimizer\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '0.334733075')\n",
      "('Epoch: ', '02', ', Loss=', '0.084679764')\n",
      "('Epoch: ', '03', ', Loss=', '0.058210979')\n",
      "('Epoch: ', '04', ', Loss=', '0.047026766')\n",
      "('Epoch: ', '05', ', Loss=', '0.038397585', ', Validation accuracy=', 0.98559999)\n",
      "('Epoch: ', '06', ', Loss=', '0.032200689')\n",
      "('Epoch: ', '07', ', Loss=', '0.027202506')\n",
      "('Epoch: ', '08', ', Loss=', '0.024449474')\n",
      "('Epoch: ', '09', ', Loss=', '0.023225185')\n",
      "('Epoch: ', '10', ', Loss=', '0.016299678', ', Validation accuracy=', 0.9896)\n",
      "('Epoch: ', '11', ', Loss=', '0.015568608')\n",
      "('Epoch: ', '12', ', Loss=', '0.014075046')\n",
      "('Epoch: ', '13', ', Loss=', '0.012065283')\n",
      "('Epoch: ', '14', ', Loss=', '0.013818121')\n",
      "('Epoch: ', '15', ', Loss=', '0.009866576', ', Validation accuracy=', 0.99299997)\n",
      "('Epoch: ', '16', ', Loss=', '0.009145677')\n",
      "('Epoch: ', '17', ', Loss=', '0.007091073')\n",
      "('Epoch: ', '18', ', Loss=', '0.008851066')\n",
      "('Epoch: ', '19', ', Loss=', '0.007410657')\n",
      "('Epoch: ', '20', ', Loss=', '0.007903185', ', Validation accuracy=', 0.9878)\n",
      "('Epoch: ', '21', ', Loss=', '0.006398531')\n",
      "('Epoch: ', '22', ', Loss=', '0.006786402')\n",
      "('Epoch: ', '23', ', Loss=', '0.004275369')\n",
      "('Epoch: ', '24', ', Loss=', '0.005407030')\n",
      "('Epoch: ', '25', ', Loss=', '0.006836152', ', Validation accuracy=', 0.99019998)\n",
      "('Epoch: ', '26', ', Loss=', '0.005705201')\n",
      "('Epoch: ', '27', ', Loss=', '0.004108396')\n",
      "('Epoch: ', '28', ', Loss=', '0.005189056')\n",
      "('Epoch: ', '29', ', Loss=', '0.004969921')\n",
      "('Epoch: ', '30', ', Loss=', '0.004383926', ', Validation accuracy=', 0.98820001)\n",
      "('Epoch: ', '31', ', Loss=', '0.004934460')\n",
      "('Epoch: ', '32', ', Loss=', '0.002727319')\n",
      "('Epoch: ', '33', ', Loss=', '0.001030852')\n",
      "('Epoch: ', '34', ', Loss=', '0.004559072')\n",
      "('Epoch: ', '35', ', Loss=', '0.007239455', ', Validation accuracy=', 0.99199998)\n",
      "('Epoch: ', '36', ', Loss=', '0.004636614')\n",
      "('Epoch: ', '37', ', Loss=', '0.001431842')\n",
      "('Epoch: ', '38', ', Loss=', '0.002369084')\n",
      "('Epoch: ', '39', ', Loss=', '0.008141571')\n",
      "('Epoch: ', '40', ', Loss=', '0.003949543', ', Validation accuracy=', 0.9928)\n",
      "('Epoch: ', '41', ', Loss=', '0.001264404')\n",
      "('Epoch: ', '42', ', Loss=', '0.002535661')\n",
      "('Epoch: ', '43', ', Loss=', '0.004705596')\n",
      "('Epoch: ', '44', ', Loss=', '0.002646992')\n",
      "('Epoch: ', '45', ', Loss=', '0.000479456', ', Validation accuracy=', 0.99199998)\n",
      "('Epoch: ', '46', ', Loss=', '0.004931494')\n",
      "('Epoch: ', '47', ', Loss=', '0.003603306')\n",
      "('Epoch: ', '48', ', Loss=', '0.003026230')\n",
      "('Epoch: ', '49', ', Loss=', '0.002332927')\n",
      "('Epoch: ', '50', ', Loss=', '0.004189530', ', Validation accuracy=', 0.99040002)\n",
      "('Epoch: ', '51', ', Loss=', '0.003937562')\n",
      "('Epoch: ', '52', ', Loss=', '0.001226550')\n",
      "('Epoch: ', '53', ', Loss=', '0.000480962')\n",
      "('Epoch: ', '54', ', Loss=', '0.004740439')\n",
      "('Epoch: ', '55', ', Loss=', '0.004106833', ', Validation accuracy=', 0.99080002)\n",
      "('Epoch: ', '56', ', Loss=', '0.001400927')\n",
      "('Epoch: ', '57', ', Loss=', '0.001977957')\n",
      "('Epoch: ', '58', ', Loss=', '0.001611727')\n",
      "('Epoch: ', '59', ', Loss=', '0.000239811')\n",
      "('Epoch: ', '60', ', Loss=', '0.000235764', ', Validation accuracy=', 0.99159998)\n",
      "('Epoch: ', '61', ', Loss=', '0.004129012')\n",
      "('Epoch: ', '62', ', Loss=', '0.005823750')\n",
      "('Epoch: ', '63', ', Loss=', '0.000873959')\n",
      "('Epoch: ', '64', ', Loss=', '0.000531080')\n",
      "('Epoch: ', '65', ', Loss=', '0.001626104', ', Validation accuracy=', 0.99159998)\n",
      "('Epoch: ', '66', ', Loss=', '0.004949070')\n",
      "('Epoch: ', '67', ', Loss=', '0.002650149')\n",
      "('Epoch: ', '68', ', Loss=', '0.003393453')\n",
      "('Epoch: ', '69', ', Loss=', '0.001388342')\n",
      "('Epoch: ', '70', ', Loss=', '0.000772466', ', Validation accuracy=', 0.99059999)\n",
      "('Epoch: ', '71', ', Loss=', '0.002649101')\n",
      "('Epoch: ', '72', ', Loss=', '0.002611062')\n",
      "('Epoch: ', '73', ', Loss=', '0.003584808')\n",
      "('Epoch: ', '74', ', Loss=', '0.001996798')\n",
      "('Epoch: ', '75', ', Loss=', '0.000457499', ', Validation accuracy=', 0.99119997)\n",
      "('Epoch: ', '76', ', Loss=', '0.000107619')\n",
      "('Epoch: ', '77', ', Loss=', '0.000025367')\n",
      "('Epoch: ', '78', ', Loss=', '0.000007383')\n",
      "('Epoch: ', '79', ', Loss=', '0.000005040')\n",
      "('Epoch: ', '80', ', Loss=', '0.000003893', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '81', ', Loss=', '0.000003127')\n",
      "('Epoch: ', '82', ', Loss=', '0.000002584')\n",
      "('Epoch: ', '83', ', Loss=', '0.000002132')\n",
      "('Epoch: ', '84', ', Loss=', '0.000001767')\n",
      "('Epoch: ', '85', ', Loss=', '0.000001465', ', Validation accuracy=', 0.99159998)\n",
      "('Epoch: ', '86', ', Loss=', '0.000001219')\n",
      "('Epoch: ', '87', ', Loss=', '0.000001017')\n",
      "('Epoch: ', '88', ', Loss=', '0.000000839')\n",
      "('Epoch: ', '89', ', Loss=', '0.000000688')\n",
      "('Epoch: ', '90', ', Loss=', '0.000000569', ', Validation accuracy=', 0.99159998)\n",
      "('Epoch: ', '91', ', Loss=', '0.000000467')\n",
      "('Epoch: ', '92', ', Loss=', '0.000000384')\n",
      "('Epoch: ', '93', ', Loss=', '0.000000313')\n",
      "('Epoch: ', '94', ', Loss=', '0.000000256')\n",
      "('Epoch: ', '95', ', Loss=', '0.000000208', ', Validation accuracy=', 0.99180001)\n",
      "('Epoch: ', '96', ', Loss=', '0.000000166')\n",
      "('Epoch: ', '97', ', Loss=', '0.000000136')\n",
      "('Epoch: ', '98', ', Loss=', '0.000000110')\n",
      "('Epoch: ', '99', ', Loss=', '0.000000088')\n",
      "('Epoch: ', '100', ', Loss=', '0.000000071', ', Validation accuracy=', 0.99180001)\n",
      "Optimization Finished! in 2116.570132 seconds\n",
      "('Accuracy on test data: ', [0.99250001])\n",
      "****************************************************************************************************\n",
      "LR=0.0001 \t- Batchsize =128 \t - AdamOptimizer\n",
      "****************************************************************************************************\n",
      "Start Training!\n",
      "('Epoch: ', '01', ', Loss=', '1.145880231')\n",
      "('Epoch: ', '02', ', Loss=', '0.297660134')\n",
      "('Epoch: ', '03', ', Loss=', '0.207907060')\n",
      "('Epoch: ', '04', ', Loss=', '0.165544858')\n",
      "('Epoch: ', '05', ', Loss=', '0.138812527', ', Validation accuracy=', 0.96499997)\n",
      "('Epoch: ', '06', ', Loss=', '0.120730496')\n",
      "('Epoch: ', '07', ', Loss=', '0.108552518')\n",
      "('Epoch: ', '08', ', Loss=', '0.098652180')\n",
      "('Epoch: ', '09', ', Loss=', '0.090535320')\n",
      "('Epoch: ', '10', ', Loss=', '0.084222748', ', Validation accuracy=', 0.97500002)\n",
      "('Epoch: ', '11', ', Loss=', '0.078142952')\n",
      "('Epoch: ', '12', ', Loss=', '0.074453516')\n",
      "('Epoch: ', '13', ', Loss=', '0.070165736')\n",
      "('Epoch: ', '14', ', Loss=', '0.066237058')\n",
      "('Epoch: ', '15', ', Loss=', '0.062903373', ', Validation accuracy=', 0.98140001)\n",
      "('Epoch: ', '16', ', Loss=', '0.060209017')\n",
      "('Epoch: ', '17', ', Loss=', '0.056782207')\n",
      "('Epoch: ', '18', ', Loss=', '0.054652749')\n",
      "('Epoch: ', '19', ', Loss=', '0.052874925')\n",
      "('Epoch: ', '20', ', Loss=', '0.050483304', ', Validation accuracy=', 0.98439997)\n",
      "('Epoch: ', '21', ', Loss=', '0.048192486')\n",
      "('Epoch: ', '22', ', Loss=', '0.046354673')\n",
      "('Epoch: ', '23', ', Loss=', '0.044425137')\n",
      "('Epoch: ', '24', ', Loss=', '0.043597523')\n",
      "('Epoch: ', '25', ', Loss=', '0.041440343', ', Validation accuracy=', 0.98640001)\n",
      "('Epoch: ', '26', ', Loss=', '0.039736577')\n",
      "('Epoch: ', '27', ', Loss=', '0.038241425')\n",
      "('Epoch: ', '28', ', Loss=', '0.037288192')\n",
      "('Epoch: ', '29', ', Loss=', '0.036747289')\n",
      "('Epoch: ', '30', ', Loss=', '0.034052418', ', Validation accuracy=', 0.9874)\n",
      "('Epoch: ', '31', ', Loss=', '0.033775570')\n",
      "('Epoch: ', '32', ', Loss=', '0.032668662')\n",
      "('Epoch: ', '33', ', Loss=', '0.031692045')\n",
      "('Epoch: ', '34', ', Loss=', '0.030579534')\n",
      "('Epoch: ', '35', ', Loss=', '0.029427264', ', Validation accuracy=', 0.98839998)\n",
      "('Epoch: ', '36', ', Loss=', '0.028391233')\n",
      "('Epoch: ', '37', ', Loss=', '0.027410484')\n",
      "('Epoch: ', '38', ', Loss=', '0.026814038')\n",
      "('Epoch: ', '39', ', Loss=', '0.025106479')\n",
      "('Epoch: ', '40', ', Loss=', '0.024868007', ', Validation accuracy=', 0.98799998)\n",
      "('Epoch: ', '41', ', Loss=', '0.024354223')\n",
      "('Epoch: ', '42', ', Loss=', '0.023217289')\n",
      "('Epoch: ', '43', ', Loss=', '0.022500762')\n",
      "('Epoch: ', '44', ', Loss=', '0.021557813')\n",
      "('Epoch: ', '45', ', Loss=', '0.020813785', ', Validation accuracy=', 0.99000001)\n",
      "('Epoch: ', '46', ', Loss=', '0.019936290')\n",
      "('Epoch: ', '47', ', Loss=', '0.019715844')\n",
      "('Epoch: ', '48', ', Loss=', '0.018630078')\n",
      "('Epoch: ', '49', ', Loss=', '0.018203988')\n",
      "('Epoch: ', '50', ', Loss=', '0.017314640', ', Validation accuracy=', 0.9892)\n",
      "('Epoch: ', '51', ', Loss=', '0.016839278')\n",
      "('Epoch: ', '52', ', Loss=', '0.016217940')\n",
      "('Epoch: ', '53', ', Loss=', '0.015446401')\n",
      "('Epoch: ', '54', ', Loss=', '0.014829095')\n",
      "('Epoch: ', '55', ', Loss=', '0.014190396', ', Validation accuracy=', 0.9896)\n",
      "('Epoch: ', '56', ', Loss=', '0.014141735')\n",
      "('Epoch: ', '57', ', Loss=', '0.013727619')\n",
      "('Epoch: ', '58', ', Loss=', '0.012492964')\n",
      "('Epoch: ', '59', ', Loss=', '0.012645164')\n",
      "('Epoch: ', '60', ', Loss=', '0.012169781', ', Validation accuracy=', 0.98979998)\n",
      "('Epoch: ', '61', ', Loss=', '0.011588894')\n",
      "('Epoch: ', '62', ', Loss=', '0.010947986')\n",
      "('Epoch: ', '63', ', Loss=', '0.010273093')\n",
      "('Epoch: ', '64', ', Loss=', '0.010512493')\n",
      "('Epoch: ', '65', ', Loss=', '0.009705119', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '66', ', Loss=', '0.009388880')\n",
      "('Epoch: ', '67', ', Loss=', '0.008722944')\n",
      "('Epoch: ', '68', ', Loss=', '0.008963664')\n",
      "('Epoch: ', '69', ', Loss=', '0.008960769')\n",
      "('Epoch: ', '70', ', Loss=', '0.008182113', ', Validation accuracy=', 0.99000001)\n",
      "('Epoch: ', '71', ', Loss=', '0.008014084')\n",
      "('Epoch: ', '72', ', Loss=', '0.007621558')\n",
      "('Epoch: ', '73', ', Loss=', '0.007144715')\n",
      "('Epoch: ', '74', ', Loss=', '0.006994748')\n",
      "('Epoch: ', '75', ', Loss=', '0.006674157', ', Validation accuracy=', 0.9874)\n",
      "('Epoch: ', '76', ', Loss=', '0.006103111')\n",
      "('Epoch: ', '77', ', Loss=', '0.006598834')\n",
      "('Epoch: ', '78', ', Loss=', '0.006481852')\n",
      "('Epoch: ', '79', ', Loss=', '0.005669183')\n",
      "('Epoch: ', '80', ', Loss=', '0.005034441', ', Validation accuracy=', 0.99040002)\n",
      "('Epoch: ', '81', ', Loss=', '0.004776861')\n",
      "('Epoch: ', '82', ', Loss=', '0.004835924')\n",
      "('Epoch: ', '83', ', Loss=', '0.005173873')\n",
      "('Epoch: ', '84', ', Loss=', '0.004461128')\n",
      "('Epoch: ', '85', ', Loss=', '0.004324440', ', Validation accuracy=', 0.98979998)\n",
      "('Epoch: ', '86', ', Loss=', '0.004058265')\n",
      "('Epoch: ', '87', ', Loss=', '0.003733395')\n",
      "('Epoch: ', '88', ', Loss=', '0.004150975')\n",
      "('Epoch: ', '89', ', Loss=', '0.004009332')\n",
      "('Epoch: ', '90', ', Loss=', '0.003353655', ', Validation accuracy=', 0.98979998)\n",
      "('Epoch: ', '91', ', Loss=', '0.003085411')\n",
      "('Epoch: ', '92', ', Loss=', '0.003109765')\n",
      "('Epoch: ', '93', ', Loss=', '0.003814062')\n",
      "('Epoch: ', '94', ', Loss=', '0.003857242')\n",
      "('Epoch: ', '95', ', Loss=', '0.002379495', ', Validation accuracy=', 0.99019998)\n",
      "('Epoch: ', '96', ', Loss=', '0.002587274')\n",
      "('Epoch: ', '97', ', Loss=', '0.002169892')\n",
      "('Epoch: ', '98', ', Loss=', '0.002888090')\n",
      "('Epoch: ', '99', ', Loss=', '0.002547430')\n",
      "('Epoch: ', '100', ', Loss=', '0.002074964', ', Validation accuracy=', 0.98760003)\n",
      "Optimization Finished! in 2289.042168 seconds\n",
      "('Accuracy on test data: ', [0.98879999])\n"
     ]
    }
   ],
   "source": [
    "print '*'*100\n",
    "print 'LR=0.001 \\t- Batchsize =128 \\t - GradientDescent'\n",
    "print '*'*100\n",
    "mnist_relu_001_128 = NeuralNetwork(LR=0.001, Iter=100, Batchsize=128, display_step= 5 ,\n",
    "                         Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_001_128.train(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "print '*'*100\n",
    "print 'LR=0.0001 \\t- Batchsize =128 \\t - GradientDescent'\n",
    "print '*'*100\n",
    "mnist_relu_0001_128 = NeuralNetwork(LR=0.0001, Iter=100, Batchsize=128, display_step= 5 ,\n",
    "                         Optimizer=tf.train.GradientDescentOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_0001_128.train(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "print '*'*100\n",
    "print 'LR=0.001 \\t- Batchsize =50 \\t - AdamOptimizer'\n",
    "print '*'*100\n",
    "mnist_relu_001_50 = NeuralNetwork(LR=0.001, Iter=100, Batchsize=50, display_step= 5 ,\n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_001_50.train(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "print '*'*100\n",
    "print 'LR=0.0001 \\t- Batchsize =50 \\t - AdamOptimizer'\n",
    "print '*'*100\n",
    "mnist_relu_0001_50 = NeuralNetwork(LR=0.0001, Iter=100, Batchsize=50, display_step= 5 ,\n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_0001_50.train(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "print '*'*100\n",
    "print 'LR=0.001 \\t- Batchsize =128 \\t - AdamOptimizer'\n",
    "print '*'*100\n",
    "mnist_relu_001_128 = NeuralNetwork(LR=0.001, Iter=100, Batchsize=128, display_step= 5 ,\n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_001_128.train(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "print '*'*100\n",
    "print 'LR=0.0001 \\t- Batchsize =128 \\t - AdamOptimizer'\n",
    "print '*'*100\n",
    "mnist_relu_0001_128 = NeuralNetwork(LR=0.0001, Iter=100, Batchsize=128, display_step= 5 ,\n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=False, Activation=tf.nn.relu)\n",
    "mnist_relu_0001_128.train(X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrain your network with SGD and AdamOptimizer and then fill the table above  : **with LR = 0.001 batch size = 128 and relu activation**\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent         |AdamOptimizer |\n",
    "| -------------        |: -------------: | ---------:   \n",
    "| Validation Accuracy  |   0.9744        |   0.9918     |      \n",
    "| Testing Accuracy     |   0.9775        |   0.9925     |       \n",
    "| Training Time        |   2437.31s      |   2116.57s   |  |  \n",
    "\n",
    "\n",
    "- Try with different learning rates for each Optimizer (0.0001 and 0.001 ) and different Batch sizes (50 and 128) for 20000 Epochs. \n",
    "The different batch size does not affect the final result much. The learning rate affect the final result. Smaller learning rate means slower convergence. In Adam optimizer the learning rate is adjusted to the current gradient so the convergence is more precise and yield better result.\n",
    "\n",
    "- We reached over 99% acuracy on the validation and the test set. The optimal parameter for this model is: Adam optimizer, batchsize = 128, LR = 0.001.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAKqCAYAAACgkz5JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOAQAAADQMeEQAAAAQMeEQwAAAAAd+/8dHAHrfNDbrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f97c0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "GD_batch_rate_acc = {'50':[88.1,97.5],'128': [87.1,97.75]} # with lr = [0.0001, 0.001]\n",
    "Adam_batch_rate_acc =  {'50':[98.79, 99.22],'128':[98.87, 99.25]}\n",
    "Training_scheme = [\"Batch=50, LR=0.001\", \"Batch=50, LR=0.0001\", \"Batch=128, LR=0.001\", \"Batch=128; LR=0.0001\"]\n",
    "GD_test_acc = [97.5, 88.1, 97.75, 87.1]\n",
    "Adam_test_acc = [99.22, 98.79, 99.25, 98.87]\n",
    "# plot the test accuracies according to the training settings\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(np.arange(len(Training_scheme))*2 - 0.25, GD_test_acc, width=0.5, label=\"Gradient Descend Optimizer\")\n",
    "plt.bar(np.arange(len(Training_scheme))*2 + 0.25, Adam_test_acc, width=0.5, label=\"Adam Optimizer\")\n",
    "plt.xticks(np.arange(len(Training_scheme))*2,Training_scheme,fontsize=20)\n",
    "plt.legend(loc=1,fontsize=20,bbox_to_anchor=(1, 1.2))\n",
    "plt.ylim(85,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient descend is always worse than the Adam optimizer, because the constant gradient prevent the optimizer to get to lower loss in the objective function, to get to the minima of the loss function a lower lr is needed and Adam can reduce the learning rate.\n",
    "\n",
    "The lower the LR the better this means that the minima of the loss function is so steep and narrow that the LR need to be so small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.2.2 </b>  What about applying a dropout layer on the Fully conntected layer and then retraining the model with the best Optimizer and parameters(Learning rate and Batsh size) obtained in *Question 2.2.1*  ? (probability to keep units=0.75). For this stage ensure that the keep prob is set to 1.0 to evaluate the \n",
    "performance of the network including all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!Dropout_Adam_0.001_50_128_test\n",
      "('Epoch: ', '01', ', Loss=', '0.447938985')\n",
      "('Epoch: ', '02', ', Loss=', '0.136061754')\n",
      "('Epoch: ', '03', ', Loss=', '0.096223509')\n",
      "('Epoch: ', '04', ', Loss=', '0.076480196')\n",
      "('Epoch: ', '05', ', Loss=', '0.065376022', ', Validation accuracy=', 0.98580003)\n",
      "('Epoch: ', '06', ', Loss=', '0.057979050')\n",
      "('Epoch: ', '07', ', Loss=', '0.051648719')\n",
      "('Epoch: ', '08', ', Loss=', '0.043890592')\n",
      "('Epoch: ', '09', ', Loss=', '0.041343244')\n",
      "('Epoch: ', '10', ', Loss=', '0.038868583', ', Validation accuracy=', 0.98860002)\n",
      "('Epoch: ', '11', ', Loss=', '0.036879315')\n",
      "('Epoch: ', '12', ', Loss=', '0.032401314')\n",
      "('Epoch: ', '13', ', Loss=', '0.029717063')\n",
      "('Epoch: ', '14', ', Loss=', '0.027466644')\n",
      "('Epoch: ', '15', ', Loss=', '0.024912257', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '16', ', Loss=', '0.025506783')\n",
      "('Epoch: ', '17', ', Loss=', '0.022327937')\n",
      "('Epoch: ', '18', ', Loss=', '0.022455513')\n",
      "('Epoch: ', '19', ', Loss=', '0.021132594')\n",
      "('Epoch: ', '20', ', Loss=', '0.019656451', ', Validation accuracy=', 0.98900002)\n",
      "('Epoch: ', '21', ', Loss=', '0.018900563')\n",
      "('Epoch: ', '22', ', Loss=', '0.018239925')\n",
      "('Epoch: ', '23', ', Loss=', '0.017471733')\n",
      "('Epoch: ', '24', ', Loss=', '0.015586050')\n",
      "('Epoch: ', '25', ', Loss=', '0.015249811', ', Validation accuracy=', 0.99059999)\n",
      "('Epoch: ', '26', ', Loss=', '0.015430736')\n",
      "('Epoch: ', '27', ', Loss=', '0.014786904')\n",
      "('Epoch: ', '28', ', Loss=', '0.014904474')\n",
      "('Epoch: ', '29', ', Loss=', '0.014545154')\n",
      "('Epoch: ', '30', ', Loss=', '0.011843947', ', Validation accuracy=', 0.991)\n",
      "('Epoch: ', '31', ', Loss=', '0.012754284')\n",
      "('Epoch: ', '32', ', Loss=', '0.012369390')\n",
      "('Epoch: ', '33', ', Loss=', '0.013857399')\n",
      "('Epoch: ', '34', ', Loss=', '0.012799347')\n",
      "('Epoch: ', '35', ', Loss=', '0.010998348', ', Validation accuracy=', 0.99000001)\n",
      "('Epoch: ', '36', ', Loss=', '0.011467674')\n",
      "('Epoch: ', '37', ', Loss=', '0.010748389')\n",
      "('Epoch: ', '38', ', Loss=', '0.009464906')\n",
      "('Epoch: ', '39', ', Loss=', '0.010977688')\n",
      "('Epoch: ', '40', ', Loss=', '0.011876675', ', Validation accuracy=', 0.99059999)\n",
      "('Epoch: ', '41', ', Loss=', '0.009454043')\n",
      "('Epoch: ', '42', ', Loss=', '0.008878208')\n",
      "('Epoch: ', '43', ', Loss=', '0.010025518')\n",
      "('Epoch: ', '44', ', Loss=', '0.009253393')\n",
      "('Epoch: ', '45', ', Loss=', '0.008039617', ', Validation accuracy=', 0.99019998)\n",
      "('Epoch: ', '46', ', Loss=', '0.008836178')\n",
      "('Epoch: ', '47', ', Loss=', '0.009922454')\n",
      "('Epoch: ', '48', ', Loss=', '0.008162007')\n",
      "('Epoch: ', '49', ', Loss=', '0.008330653')\n",
      "('Epoch: ', '50', ', Loss=', '0.007217483', ', Validation accuracy=', 0.99059999)\n",
      "Optimization Finished! in 1231.374899 seconds\n",
      "('Accuracy on test data: ', [0.99239999])\n"
     ]
    }
   ],
   "source": [
    "# your implementaion goas here\n",
    "mnist_dropout = NeuralNetwork(LR=0.001, Iter=50, Batchsize=128, \n",
    "                         Optimizer=tf.train.AdamOptimizer, IsDropOut=True, Activation=tf.nn.relu,NN_Name=\"Dropout_Adam_0.001_50_128_test\")\n",
    "mnist_dropout.train(X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Your comments go here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Tensorflow 2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
